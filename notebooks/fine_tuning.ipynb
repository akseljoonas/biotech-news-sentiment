{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "397273d8a6e1aae7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T13:11:23.664156Z",
     "start_time": "2024-09-11T13:11:10.901679Z"
    },
    "id": "397273d8a6e1aae7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cpu\n",
      "False\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "device = (\n",
    "    torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    ")\n",
    "# if it is not MPS, try CUDA\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else device\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    # random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # If using GPUs\n",
    "\n",
    "set_seed(4242)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d241505041a6a8",
   "metadata": {
    "collapsed": false,
    "id": "b1d241505041a6a8"
   },
   "source": [
    "### Task 1: Sentiment Analysis\n",
    "Sentiment Analysis is the task of classifying a given text into positive, negative, or neutral sentiments.\n",
    "\n",
    "In this example, we will use a pre-trained model for sentiment analysis. The `pipeline` will automatically download and load a model that has been pre-trained on a large dataset to perform this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c08c573b7d61fca0",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.666148400Z"
    },
    "id": "c08c573b7d61fca0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis Result: [{'label': 'LABEL_1', 'score': 0.5116071701049805}]\n"
     ]
    }
   ],
   "source": [
    "#model_name = \"ncbi/MedCPT-Article-Encoder\"\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model_name, device=device)\n",
    "result = classifier(\n",
    "    \"The development of a recombinant polyclonal antibody therapy for COVID-19 by GigaGen represents an early-stage positive news in response to a global health crisis. However, such initiatives often come with high risk and uncertainty given the complexity and time required for clinical trials and approval processes. Additionally, competition in the COVID-19 treatment space is intense, with many companies pursuing similar therapies. These factors make it essential to remain cautious, monitoring further developments and data closely.\"\n",
    ")\n",
    "\n",
    "print(f\"Sentiment Analysis Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23025de743370a0f",
   "metadata": {
    "collapsed": false,
    "id": "23025de743370a0f"
   },
   "source": [
    "# 3. Fine-Tuning Pre-trained Models\n",
    "While the pre-trained models provided by Hugging Face are powerful, you may want to fine-tune them for a specific task or dataset.\n",
    "\n",
    "Fine-tuning involves taking a pre-trained model and training it further on your own data. This can improve the model’s performance for specific use cases.\n",
    "\n",
    "For this section, we’ll load the IMDB dataset (which contains movie reviews) and fine-tune a pre-trained model for sentiment classification.\n",
    "\n",
    "### Step 1: Load Dataset\n",
    "We'll use Hugging Face's datasets library to load the IMDB dataset.\n",
    "\n",
    "Datasets from the dataset library often come with pre-defined splits of the data, such as `train` and `test` sets.\n",
    "\n",
    "It is possible to filter or slice datasets to focus on specific subsets of the data, using the `select` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b02c5a5d660751f6",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.684099300Z"
    },
    "id": "b02c5a5d660751f6"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Features, ClassLabel, Value\n",
    "import os, sys\n",
    "\n",
    "currentdir = os.getcwd()\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0, parentdir)\n",
    "data_file_path = os.path.join(parentdir, \"data/processed/finetuning-3label.csv\")\n",
    "if not os.path.exists(data_file_path):\n",
    "    raise FileNotFoundError(f\"The file {data_file_path} does not exist!\")\n",
    "\n",
    "# Define the features\n",
    "features = Features(\n",
    "    {\n",
    "        \"text\": Value(\"string\"),\n",
    "        \"labels\": ClassLabel(names=[\"0\", \"1\", \"2\"]),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Load the dataset with the specified features\n",
    "dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files=data_file_path,\n",
    "    features=features,\n",
    ")\n",
    "\n",
    "\n",
    "#Now you can stratify by 'labels'\n",
    "#split_dataset = dataset[\"train\"].train_test_split(\n",
    "#    test_size=0.1, stratify_by_column=\"labels\"\n",
    "#)\n",
    "#train_dataset = split_dataset[\"train\"]\n",
    "#test_dataset = split_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a columns \"id\" with a unique row id\n",
    "dataset[\"train\"] = dataset[\"train\"].add_column(\"id\", range(len(dataset[\"train\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'labels', 'id'],\n",
      "        num_rows: 3880\n",
      "    })\n",
      "})\n",
      "{'text': 'GigaGen to Present at the Inaugural LifeSci Partners Private Company Virtual Summer Symposium SOUTH SAN FRANCISCO, Calif., July  29, 2020  (GLOBE NEWSWIRE) -- GigaGen Inc., a biotechnology company advancing transformative antibody drugs for infectious diseases, transplant rejection and checkpoint resistant cancers, today announced that it will participate in the LifeSci Partners Private Company Virtual Summer Symposium, taking place on August 4-5, 2020.', 'labels': 1, 'id': 1}\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "print(dataset[\"train\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "labels = np.array(dataset[\"train\"][\"labels\"])\n",
    "\n",
    "# Generate indices for stratified split\n",
    "indices = np.arange(len(labels))\n",
    "train_indices, test_indices = train_test_split(\n",
    "    indices,\n",
    "    test_size=0.1,\n",
    "    stratify=labels,\n",
    "    random_state=42,  # Ensures reproducibility\n",
    ")\n",
    "\n",
    "# Create stratified train and test datasets, remember its a pandas dataframe\n",
    "train_dataset = dataset[\"train\"].select(train_indices)\n",
    "test_dataset = dataset[\"train\"].select(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "573857f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Pacira Appoints Frank D. Lee as Chief Executive Officer -- Transformational Leader Brings Three Decades of Global Experience in Pharmaceutical and Biotechnology Product Development and Commercialization\\xa0-- -- Paul J. Hastings Named Chair of the Board --',\n",
       " 'labels': 1,\n",
       " 'id': 989}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming labels is a numpy array of your dataset labels\n",
    "labels = np.array(dataset[\"train\"][\"labels\"])\n",
    "label_counts = np.bincount(labels)\n",
    "label_percentages = label_counts / label_counts.sum() * 100  # Convert counts to percentages\n",
    "\n",
    "# Define label names\n",
    "label_names = [\"Negative\", \"Neutral\", \"Positive\"]\n",
    "\n",
    "# Plot the histogram\n",
    "plt.bar(range(len(label_percentages)), label_percentages)\n",
    "plt.xlabel(\"Label\")  # Set the x-axis label\n",
    "plt.ylabel(\"Percentage (%)\")  # Set the y-axis label\n",
    "plt.title(\"Percentage Distribution of Labels\")  # Set the title\n",
    "plt.xticks(range(len(label_percentages)), label_names)  # Map labels to names\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Distribution of Text Lengths')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#now plot the distribution of text lengths by words\n",
    "\n",
    "text_lengths = [len(text.split()) for text in dataset[\"train\"][\"text\"]]\n",
    "plt.hist(text_lengths, bins=20)\n",
    "plt.xlabel(\"Text Length (Words)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Text Lengths\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404f2e557f938c88",
   "metadata": {
    "collapsed": false,
    "id": "404f2e557f938c88"
   },
   "source": [
    "### Step 2: Tokenize the Dataset\n",
    "The dataset needs to be tokenized before it can be fed into the model. Tokenization converts the text data into numerical format (tokens) that the model can process.\n",
    "\n",
    "We'll use the `AutoTokenizer` class from HuggingFace to tokenize the data. The `AutoTokenizer` class automatically selects the appropriate tokenizer for the model based on the `model_name`.\n",
    "\n",
    "Tokenization or transformation of the dataset can be done using the `map` method, which applies a function to all the elements of the dataset. This is easily done by defining a function that tokenizes the text data and then applying it to the dataset. When `batched=True`, the function will be applied to batches of data, which can improve performance by applying the function in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76bfbf6e30ed6c40",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.687091400Z"
    },
    "id": "76bfbf6e30ed6c40"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer, padding=\"max_length\", max_length=512\n",
    ")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # print(examples[\"text\"][0])\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6454d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'labels', 'id', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 388\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20413a582b139a0e",
   "metadata": {
    "collapsed": false,
    "id": "20413a582b139a0e"
   },
   "source": [
    "### Step 3: Load a Pre-trained Model\n",
    "Now that the data is tokenized, we'll load a pre-trained model that we'll fine-tune for sentiment classification.\n",
    "\n",
    "We'll use distilbert-base-uncased for this task.\n",
    "\n",
    "We need to import `AutoModelForSequenceClassification` for that. The key feature of this class is that it adds a classification head on top of the pre-trained transformer model to allow it to classify sequences into one or more categories (e.g., positive/negative sentiment, spam/ham, etc.). The `from_pretrained` method loads the pre-trained model with the specified configuration. The `num_labels` parameter specifies the number of labels in the classification task (binary in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c217ddcb27c998d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.689086Z"
    },
    "id": "7c217ddcb27c998d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels=3, # ignore_mismatched_sizes=True\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b466e12de8bfef",
   "metadata": {
    "collapsed": false,
    "id": "8b466e12de8bfef"
   },
   "source": [
    "### Step 4: Set Up the Trainer\n",
    "Hugging Face provides the Trainer class to help with the training and fine-tuning of models. We need to set up the trainer by providing the model, training arguments, and the datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91b8db4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average=\"weighted\", labels=[0, 1, 2]\n",
    "    )\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bae5b805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: tensor([2.8055, 0.4608, 2.1133])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(dataset[\"train\"][\"labels\"]),\n",
    "    y=dataset[\"train\"][\"labels\"],\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "print(\"Class Weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa808c9",
   "metadata": {},
   "source": [
    "***Loss function***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d052821d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Define custom loss function\n",
    "class WeightedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, class_weights):\n",
    "        super(WeightedCrossEntropyLoss, self).__init__()\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        return F.cross_entropy(logits, labels, weight=self.class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85519e83",
   "metadata": {},
   "source": [
    "***Gradual unfreezing***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1917e05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradually_unfreeze(model, freeze_epoch, total_epochs):\n",
    "    total_layers = len(model.base_model.encoder.layer)\n",
    "    layers_to_unfreeze = int((freeze_epoch / total_epochs) * total_layers * 1.2)\n",
    "    layers_to_unfreeze = min(layers_to_unfreeze, total_layers)\n",
    "    \n",
    "    # Unfreeze the layers progressively from the bottom (earlier layers)\n",
    "    for i in range(total_layers - layers_to_unfreeze, total_layers):\n",
    "        for param in model.base_model.encoder.layer[i].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    print(f\"Epoch {freeze_epoch}: Unfreezing {layers_to_unfreeze}/{total_layers} layers.\")\n",
    "\n",
    "\n",
    "def freeze_all_layers(model):\n",
    "    # Freeze all transformer layers except the final classification layer\n",
    "    for param in model.base_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Unfreeze the final classifier layer\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5dfd82",
   "metadata": {},
   "source": [
    "**System metric tracking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "41234ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for tracking system metrics during training \n",
    "import time\n",
    "import psutil\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "def get_gpu_memory_usage():\n",
    "    # Run nvidia-smi to get both used and total memory\n",
    "    result = subprocess.run(\n",
    "        [\"nvidia-smi\", \"--query-gpu=memory.used,memory.total\", \"--format=csv,nounits,noheader\"],\n",
    "        stdout=subprocess.PIPE\n",
    "    )\n",
    "    used_memory, total_memory = result.stdout.decode(\"utf-8\").strip().split(\"\\n\")[0].split(\", \")\n",
    "\n",
    "    return int(used_memory), int(total_memory)\n",
    "\n",
    "# Pass the model type as a string\n",
    "def track_performance(model_type):\n",
    "    process = psutil.Process(os.getpid())\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    trainer.train()\n",
    "\n",
    "    # Track memory and time\n",
    "    memory_usage = process.memory_info().rss / (1024 * 1024)\n",
    "    gpu_memory_used, gpu_memory_total = get_gpu_memory_usage()\n",
    "    end_time = time.time()\n",
    "\n",
    "    time_elapsed = end_time - start_time\n",
    "\n",
    "    print(f\"{model_type} - Time taken: {time_elapsed:.2f} seconds\")\n",
    "    print(f\"{model_type} - CPU Memory usage: {memory_usage:.2f} MB\")\n",
    "    print(f\"{model_type} - GPU Memory usage: {gpu_memory_used} MB / {gpu_memory_total} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78269d18",
   "metadata": {},
   "source": [
    "#### Defining the custom trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3b17cfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "# Define custom trainer\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights, optimizer=None, num_training_steps=None, total_epochs=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "        self.optimizer = optimizer\n",
    "        self.num_training_steps = num_training_steps\n",
    "        self.total_epochs = total_epochs\n",
    "        self.freeze_epoch = 0\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):  # Added **kwargs to handle required arguments unused by our loss\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # Move class_weights to the same device as logits\n",
    "        loss_fct = WeightedCrossEntropyLoss(self.class_weights.to(logits.device))\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "    def create_scheduler(self, num_training_steps=None, optimizer=None): # Slanted Triangular Learning Rate\n",
    "        if self.lr_scheduler is None:\n",
    "            num_training_steps = self.num_training_steps or (\n",
    "                len(self.train_dataset) // self.args.per_device_train_batch_size * self.args.num_train_epochs\n",
    "            )\n",
    "            self.lr_scheduler = OneCycleLR(\n",
    "                optimizer=optimizer,\n",
    "                max_lr=1e-3,  # Peak learning rate\n",
    "                total_steps=num_training_steps,\n",
    "                pct_start=0.2,  # Fraction of steps to increase LR\n",
    "                anneal_strategy='linear',  # Linear decrease after peak\n",
    "            )\n",
    "        return self.lr_scheduler\n",
    "\n",
    "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
    "        # Gradually unfreeze layers based on the current epoch\n",
    "        gradually_unfreeze(self.model, self.freeze_epoch, self.total_epochs)\n",
    "        self.freeze_epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c67a47",
   "metadata": {},
   "source": [
    "***Defining the training callback***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "746d76ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class TrainingMonitorCallback(TrainerCallback):\n",
    "    \"\"\" Tracks the learning rate and gradual unfreezing\"\"\"\n",
    "    def __init__(self, model, total_layers, plot_save_path=None):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.total_layers = total_layers\n",
    "        self.lrs = []\n",
    "        self.plot_save_path = plot_save_path\n",
    "\n",
    "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
    "        \"\"\"Called at the beginning of each epoch\"\"\"\n",
    "        # Calculate how many layers are unfrozen based on the current epoch\n",
    "        gradually_unfreeze(self.model, state.epoch, args.num_train_epochs)\n",
    "        unfrozen_layers = sum(\n",
    "            1 for i in range(self.total_layers)\n",
    "            if any(param.requires_grad for param in self.model.base_model.encoder.layer[i].parameters())\n",
    "        )\n",
    "\n",
    "        # Print the number of unfrozen layers at the start of the epoch\n",
    "        print(f\"Epoch {state.epoch}: {unfrozen_layers}/{self.total_layers} encoder layers are unfrozen.\")\n",
    "\n",
    "    def on_step_end(self, args, state, control, optimizer, **kwargs):\n",
    "        \"\"\"Called at the end of each training step\"\"\"\n",
    "        if optimizer is not None:  # Safety check\n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "            self.lrs.append(lr)\n",
    "            \n",
    "    def plot_learning_rate(self):\n",
    "        \"\"\"Plot the learning rate history\"\"\"\n",
    "        if len(self.lrs) > 0:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(self.lrs)\n",
    "            plt.title('Learning Rate over Training Steps')\n",
    "            plt.xlabel('Training Step')\n",
    "            plt.ylabel('Learning Rate')\n",
    "            plt.grid(True)\n",
    "            if self.plot_save_path:\n",
    "                plt.savefig(self.plot_save_path)\n",
    "            else:\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230cef97",
   "metadata": {},
   "source": [
    "#### Trainer Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "314f8aad08895c6e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.691080400Z"
    },
    "id": "314f8aad08895c6e"
   },
   "outputs": [],
   "source": [
    "# Create the callback\n",
    "tracker = TrainingMonitorCallback(model, total_layers=len(model.base_model.encoder.layer))\n",
    "custom_optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.001)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  # Output directory\n",
    "    eval_strategy=\"epoch\",  # Evaluate after each epoch\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,  # Batch size for training\n",
    "    per_device_eval_batch_size=8,  # Batch size for evaluation\n",
    "    gradient_accumulation_steps=2,  # Gradient accumulation steps FOR MEMORY EFFICIENCY \n",
    "    num_train_epochs=10,  # Number of epochs\n",
    "    load_best_model_at_end=True,  # Load the best model at the end\n",
    "    metric_for_best_model=\"f1\",  # Use F1 score to select the best model\n",
    "    save_total_limit=1,  # Limit the total amount of checkpoints\n",
    ")\n",
    "\n",
    "num_training_steps = (\n",
    "    (len(tokenized_train) // training_args.per_device_train_batch_size)\n",
    "    // training_args.gradient_accumulation_steps\n",
    ") * training_args.num_train_epochs\n",
    "\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train, \n",
    "    eval_dataset=tokenized_test,  \n",
    "    compute_metrics=compute_metrics,\n",
    "    class_weights=class_weights,  \n",
    "    data_collator=data_collator,\n",
    "    optimizer=custom_optimizer,\n",
    "    num_training_steps=num_training_steps,\n",
    "    callbacks=[tracker],\n",
    "    total_epochs=training_args.num_train_epochs,\n",
    ")\n",
    "\n",
    "trainer.optimizer = custom_optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4205eac4d06f8ae5",
   "metadata": {
    "collapsed": false,
    "id": "4205eac4d06f8ae5"
   },
   "source": [
    "### Step 5: Fine-tune the Model\n",
    "Now that the trainer is set up, we can start the fine-tuning process.\n",
    "\n",
    "Run the following cell to fine-tune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3c3125c17af30c4b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.693075300Z"
    },
    "id": "3c3125c17af30c4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Unfreezing 0/12 layers.\n",
      "Epoch 0: 0/12 encoder layers are unfrozen.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='2180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   3/2180 00:03 < 2:00:58, 0.30 it/s, Epoch 0.01/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Call this function before training starts to freeze all layers\u001b[39;00m\n\u001b[0;32m      2\u001b[0m freeze_all_layers(model)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtrack_performance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfull-training\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[79], line 23\u001b[0m, in \u001b[0;36mtrack_performance\u001b[1;34m(model_type)\u001b[0m\n\u001b[0;32m     19\u001b[0m process \u001b[38;5;241m=\u001b[39m psutil\u001b[38;5;241m.\u001b[39mProcess(os\u001b[38;5;241m.\u001b[39mgetpid())\n\u001b[0;32m     21\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 23\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Track memory and time\u001b[39;00m\n\u001b[0;32m     26\u001b[0m memory_usage \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mmemory_info()\u001b[38;5;241m.\u001b[39mrss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m)\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2169\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2531\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2524\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2525\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2527\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2528\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2529\u001b[0m )\n\u001b[0;32m   2530\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2531\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2534\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2535\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2536\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2537\u001b[0m ):\n\u001b[0;32m   2538\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2539\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3678\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3676\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs)\n\u001b[0;32m   3677\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3678\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3680\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   3681\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3682\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3683\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   3684\u001b[0m ):\n",
      "Cell \u001b[1;32mIn[80], line 16\u001b[0m, in \u001b[0;36mCustomTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, **kwargs)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, inputs, return_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):  \u001b[38;5;66;03m# Added **kwargs to handle required arguments unused by our loss\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# Move class_weights to the same device as logits\u001b[39;00m\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1665\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1657\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1658\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1659\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1660\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1661\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1662\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1663\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1665\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1671\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1672\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1673\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1675\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1677\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1679\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    686\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    692\u001b[0m         output_attentions,\n\u001b[0;32m    693\u001b[0m     )\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    575\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    582\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    584\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 585\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    592\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    594\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:515\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    507\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    513\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 515\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    524\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    525\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440\u001b[0m, in \u001b[0;36mBertSdpaSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;66;03m# We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\u001b[39;00m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;66;03m# in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;66;03m# The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;66;03m# a causal mask in case tgt_len == 1.\u001b[39;00m\n\u001b[0;32m    436\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    437\u001b[0m     \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention \u001b[38;5;129;01mand\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tgt_len \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    438\u001b[0m )\n\u001b[1;32m--> 440\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout_prob\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    449\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    450\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(bsz, tgt_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_head_size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Call this function before training starts to freeze all layers\n",
    "freeze_all_layers(model)\n",
    "track_performance(\"full-training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0579ba81",
   "metadata": {},
   "source": [
    "### LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b8ee0cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "lora_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels=3, # ignore_mismatched_sizes=True\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cb9f6a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,284,867 || all params: 110,666,502 || trainable%: 1.1610\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32, \n",
    "    lora_alpha=32,  \n",
    "    lora_dropout=0.1, \n",
    "    bias=\"all\",\n",
    "    task_type=\"SEQ_CLS\"  # sequence classification\n",
    ")\n",
    "\n",
    "# Wrap the model with LoRA configuration\n",
    "lora_model = get_peft_model(lora_model, lora_config)\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ae4cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_optimizer = torch.optim.AdamW(lora_model.parameters(), lr=2e-5, weight_decay=0.001)\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train, \n",
    "    eval_dataset=tokenized_test,  \n",
    "    compute_metrics=compute_metrics,\n",
    "    class_weights=class_weights,  \n",
    "    data_collator=data_collator,\n",
    "    optimizer=custom_optimizer,\n",
    "    num_training_steps=num_training_steps,\n",
    "    total_epochs=training_args.num_train_epochs,\n",
    ")\n",
    "\n",
    "trainer.optimizer = custom_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "782a24cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BertForSequenceClassification.forward() got an unexpected keyword argument 'decoder_input_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[90], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Learn without gradual unfreezing\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrack_performance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLoRA\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[79], line 23\u001b[0m, in \u001b[0;36mtrack_performance\u001b[1;34m(model_type)\u001b[0m\n\u001b[0;32m     19\u001b[0m process \u001b[38;5;241m=\u001b[39m psutil\u001b[38;5;241m.\u001b[39mProcess(os\u001b[38;5;241m.\u001b[39mgetpid())\n\u001b[0;32m     21\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 23\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Track memory and time\u001b[39;00m\n\u001b[0;32m     26\u001b[0m memory_usage \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mmemory_info()\u001b[38;5;241m.\u001b[39mrss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m)\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2169\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2531\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2524\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2525\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2527\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2528\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2529\u001b[0m )\n\u001b[0;32m   2530\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2531\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2534\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2535\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2536\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2537\u001b[0m ):\n\u001b[0;32m   2538\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2539\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3676\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3674\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m   3675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_accepts_loss_kwargs:\n\u001b[1;32m-> 3676\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3677\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3678\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs, num_items_in_batch\u001b[38;5;241m=\u001b[39mnum_items_in_batch)\n",
      "Cell \u001b[1;32mIn[80], line 16\u001b[0m, in \u001b[0;36mCustomTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, **kwargs)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, inputs, return_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):  \u001b[38;5;66;03m# Added **kwargs to handle required arguments unused by our loss\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# Move class_weights to the same device as logits\u001b[39;00m\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\peft\\peft_model.py:1521\u001b[0m, in \u001b[0;36mPeftModelForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mpeft_type \u001b[38;5;241m==\u001b[39m PeftType\u001b[38;5;241m.\u001b[39mPOLY:\n\u001b[0;32m   1520\u001b[0m             kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m task_ids\n\u001b[1;32m-> 1521\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1523\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1524\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1525\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1526\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1527\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1528\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1530\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1532\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[0;32m   1533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1534\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:197\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[1;32m--> 197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\peft\\peft_model.py:1521\u001b[0m, in \u001b[0;36mPeftModelForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mpeft_type \u001b[38;5;241m==\u001b[39m PeftType\u001b[38;5;241m.\u001b[39mPOLY:\n\u001b[0;32m   1520\u001b[0m             kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m task_ids\n\u001b[1;32m-> 1521\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1523\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1524\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1525\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1526\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1527\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1528\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1530\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1532\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[0;32m   1533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1534\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping similar frames: Module._call_impl at line 1747 (1 times), Module._wrapped_call_impl at line 1736 (1 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:197\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[1;32m--> 197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[1;31m[... skipping similar frames: Module._call_impl at line 1747 (5 times), Module._wrapped_call_impl at line 1736 (5 times), PeftModelForSequenceClassification.forward at line 1521 (5 times), BaseTuner.forward at line 197 (5 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\peft\\peft_model.py:1521\u001b[0m, in \u001b[0;36mPeftModelForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mpeft_type \u001b[38;5;241m==\u001b[39m PeftType\u001b[38;5;241m.\u001b[39mPOLY:\n\u001b[0;32m   1520\u001b[0m             kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m task_ids\n\u001b[1;32m-> 1521\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1523\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1524\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1525\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1526\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1527\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1528\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1530\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1532\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[0;32m   1533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1534\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:197\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[1;32m--> 197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\peft\\peft_model.py:1994\u001b[0m, in \u001b[0;36mPeftModelForSeq2SeqLM.forward\u001b[1;34m(self, input_ids, attention_mask, inputs_embeds, decoder_input_ids, decoder_attention_mask, decoder_inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[0;32m   1992\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1993\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[1;32m-> 1994\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1995\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1996\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1997\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1998\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1999\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2000\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2001\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2002\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2003\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2004\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2005\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2006\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2008\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[0;32m   2009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoder_attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2010\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:197\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[1;32m--> 197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: BertForSequenceClassification.forward() got an unexpected keyword argument 'decoder_input_ids'"
     ]
    }
   ],
   "source": [
    "# Learn without gradual unfreezing\n",
    "track_performance(\"LoRA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197d2fb4351c32ea",
   "metadata": {
    "collapsed": false,
    "id": "197d2fb4351c32ea"
   },
   "source": [
    "### Step 6: Evaluate the Model\n",
    "After training, we can evaluate the model’s performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d341d66e17736303",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.695070100Z"
    },
    "id": "d341d66e17736303"
   },
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation Results: {eval_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3e9efa96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tracker.plot_learning_rate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1UJ8GcJSPhmt",
   "metadata": {
    "id": "1UJ8GcJSPhmt"
   },
   "source": [
    "### Step 7: Try out model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZTssehqcPd8R",
   "metadata": {
    "id": "ZTssehqcPd8R"
   },
   "outputs": [],
   "source": [
    "input_string = \"BioVie Announces Alignment with FDA on Clinical Trial to Assess Bezisterim in Parkinson’s Disease,SUNRISE-PD to evaluate the effect of bezisterim (NE3107) on motor and non-motor symptoms in ~60 patients with Parkinson’s disease who are naïve to carbidopa/levodopa,SUNRISE-PD to evaluate the effect of bezisterim (NE3107) on motor and non-motor symptoms in ~60 patients with Parkinson’s disease who are naïve to carbidopa/levodopa\"\n",
    "\n",
    "# Tokenize the input string\n",
    "inputs = tokenizer(input_string, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Get predictions (logits)\n",
    "with torch.no_grad():  # Disable gradient computation since we're just doing inference\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "print(logits)\n",
    "predicted_label = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "\n",
    "print(f\"Predicted label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53007a617ec9423",
   "metadata": {
    "collapsed": false,
    "id": "53007a617ec9423"
   },
   "source": [
    "### Step 8. Saving the Fine-tuned Model\n",
    "After training, it is often useful to save the fine-tuned model, so you can use it later without needing to re-train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e5c82ccd0913cc",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.699060Z"
    },
    "id": "a4e5c82ccd0913cc"
   },
   "outputs": [],
   "source": [
    "model.push_to_hub(\n",
    "    f\"akseljoonas/MedCPT-Article-Encoder-finetuned-{eval_results['f1']:.3f}\"\n",
    ")\n",
    "tokenizer.push_to_hub(\n",
    "    f\"akseljoonas/MedCPT-Article-Encoder-finetuned-{eval_results['f1']:.3f}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
