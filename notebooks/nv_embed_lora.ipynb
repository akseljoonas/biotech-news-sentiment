{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397273d8a6e1aae7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T13:11:23.664156Z",
     "start_time": "2024-09-11T13:11:10.901679Z"
    },
    "id": "397273d8a6e1aae7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoModel, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "\n",
    "device = (\n",
    "    torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    ")\n",
    "# if it is not MPS, try CUDA\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else device\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f2711ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    # random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # If using GPUs\n",
    "\n",
    "set_seed(4242)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1258d8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NVEmbedForSequenceClassification(nn.Module):\n",
    "    \"\"\"\n",
    "    Wrap NV-Embed-v2 to produce embeddings, then classify using a linear head.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name=\"nvidia/NV-Embed-v2\", num_labels=5, instruction=\"\"):\n",
    "        super().__init__()\n",
    "        self.instruction = instruction\n",
    "        self.model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "        # Freeze config if needed\n",
    "        # self.model.config.use_cache = False\n",
    "\n",
    "        # Classification head on top of 4096-dim embeddings (NV-Embed-v2 default).\n",
    "        self.classifier = nn.Linear(4096, num_labels)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        text=None,\n",
    "        labels=None,\n",
    "        max_length=2048,\n",
    "        use_cache=False,\n",
    "        # The Trainer will usually pass arguments like \"input_ids\", \"attention_mask\"\n",
    "        # but we won't use them directly. We'll rely on \"text\" from the collator.\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        text: List of strings in the batch.\n",
    "        labels: Tensor of shape [batch_size], for classification.\n",
    "        \"\"\"\n",
    "        # 1) Produce embeddings:\n",
    "        embeddings = self.model.encode(\n",
    "            text,\n",
    "            instruction=self.instruction,\n",
    "            max_length=max_length,\n",
    "            use_cache=use_cache,\n",
    "        )  # [batch_size, 4096]\n",
    "\n",
    "        # 2) (Optional) normalize the embeddings:\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "        # 3) Classification:\n",
    "        logits = self.classifier(embeddings)  # [batch_size, num_labels]\n",
    "\n",
    "        return {\"logits\": logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7186ba46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Features, ClassLabel, Value\n",
    "\n",
    "# Suppose your CSV has columns [\"text\",\"labels\"], 5 classes in total\n",
    "# Adjust data_file_path accordingly\n",
    "data_file_path = \"data/processed/finetuning_5_labels_topic_pruned.csv\"\n",
    "\n",
    "features = Features(\n",
    "    {\n",
    "        \"text\": Value(\"string\"),\n",
    "        \"labels\": ClassLabel(names=[\"0.0\", \"1.0\", \"2.0\", \"3.0\", \"4.0\"]),\n",
    "    }\n",
    ")\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=data_file_path, features=features)\n",
    "\n",
    "# We only have a \"train\" split from the CSV, so let's do our own train/test split.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_all = dataset[\"train\"].to_pandas()\n",
    "train_df, test_df = train_test_split(\n",
    "    df_all, test_size=0.1, stratify=df_all[\"labels\"], random_state=42\n",
    ")\n",
    "\n",
    "# Convert back to Dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "test_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482a3055",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NVEmbedCollator:\n",
    "    \"\"\"\n",
    "    Custom collator that returns text as a list of strings plus the labels.\n",
    "    This is crucial so that inside the model.forward(...), we can call model.encode(text=...).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, label_name=\"labels\"):\n",
    "        self.label_name = label_name\n",
    "\n",
    "    def __call__(self, features):\n",
    "        # features is a list of dicts: [{\"text\": ..., \"labels\": ...}, ...]\n",
    "        texts = [f[\"text\"] for f in features]\n",
    "        labels = [f[self.label_name] for f in features]\n",
    "\n",
    "        # Convert labels to tensor\n",
    "        labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "        # Return dictionary that HF Trainer can pass to the model\n",
    "        # Notice \"text\" is a list of strings, \"labels\" is a tensor\n",
    "        batch = {\"text\": texts, \"labels\": labels_tensor}\n",
    "        return batch\n",
    "\n",
    "\n",
    "collator = NVEmbedCollator(label_name=\"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e62504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instruction for the model\n",
    "task_instructions = (\n",
    "    \"Given a biotech press release, classify it into 5 categories (0..4).\"\n",
    ")\n",
    "\n",
    "num_labels = 5\n",
    "base_model = NVEmbedForSequenceClassification(\n",
    "    model_name=\"nvidia/NV-Embed-v2\",\n",
    "    num_labels=num_labels,\n",
    "    instruction=task_instructions,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf55f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA config for sequence classification\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS,  # \"SEQ_CLS\" or \"CAUSAL_LM\" etc.\n",
    ")\n",
    "\n",
    "lora_model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ee51f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric_accuracy = evaluate.load(\"accuracy\")\n",
    "metric_f1 = evaluate.load(\"f1\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    acc = metric_accuracy.compute(references=labels, predictions=predictions)\n",
    "    f1_val = metric_f1.compute(\n",
    "        references=labels, predictions=predictions, average=\"weighted\"\n",
    "    )\n",
    "    return {\"accuracy\": acc[\"accuracy\"], \"f1\": f1_val[\"f1\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68acd2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results-nv-embed-lora\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",  # Turn off W&B or HF logs\n",
    "    fp16=True if device.type == \"cuda\" else False,\n",
    "    # For large embeddings you may want gradient_accumulation_steps, etc.\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d544be",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a3b5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = trainer.evaluate()\n",
    "print(\"Evaluation:\", eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87ad8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = [\n",
    "    \"BioVie Announces Alignment with FDA on Clinical Trial to Assess Bezisterim in Parkinsonâ€™s Disease...\",\n",
    "    \"The development of a recombinant polyclonal antibody therapy for COVID-19 by GigaGen...\",\n",
    "]\n",
    "\n",
    "# Prepare batch manually\n",
    "batch_for_inference = {\n",
    "    \"text\": test_texts,\n",
    "    \"labels\": torch.zeros(len(test_texts), dtype=torch.long),  # dummy\n",
    "}\n",
    "\n",
    "# Move to device if needed\n",
    "for_inference = {\n",
    "    \"text\": batch_for_inference[\"text\"],\n",
    "    \"labels\": batch_for_inference[\"labels\"].to(device),\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = trainer.model(**for_inference)\n",
    "logits = outputs[\"logits\"]\n",
    "preds = torch.argmax(logits, dim=-1)\n",
    "print(\"Predicted labels:\", preds.tolist())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "news-sentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
