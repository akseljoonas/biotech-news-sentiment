{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c6ed236",
   "metadata": {},
   "source": [
    "# Fine tuning with LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e0356a",
   "metadata": {},
   "source": [
    "### This code is underdeveloped as the idea of PEFT has been dropped in this research due to the complexity of the customized training process, leading to no improvements in both memory and time complexity. This code runs, but has not been maintained and might contain minor bugs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397273d8a6e1aae7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T13:11:23.664156Z",
     "start_time": "2024-09-11T13:11:10.901679Z"
    },
    "id": "397273d8a6e1aae7"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "device = (\n",
    "    torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    ")\n",
    "# if it is not MPS, try CUDA\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else device\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    # random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # If using GPUs\n",
    "\n",
    "set_seed(4242)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d241505041a6a8",
   "metadata": {
    "collapsed": false,
    "id": "b1d241505041a6a8"
   },
   "source": [
    "### Task 1: Sentiment Analysis\n",
    "Sentiment Analysis is the task of classifying a given text into positive, negative, or neutral sentiments.\n",
    "\n",
    "In this example, we will use a pre-trained model for sentiment analysis. The `pipeline` will automatically download and load a model that has been pre-trained on a large dataset to perform this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08c573b7d61fca0",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.666148400Z"
    },
    "id": "c08c573b7d61fca0"
   },
   "outputs": [],
   "source": [
    "#model_name = \"ncbi/MedCPT-Article-Encoder\"\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model_name, device=device)\n",
    "result = classifier(\n",
    "    \"The development of a recombinant polyclonal antibody therapy for COVID-19 by GigaGen represents an early-stage positive news in response to a global health crisis. However, such initiatives often come with high risk and uncertainty given the complexity and time required for clinical trials and approval processes. Additionally, competition in the COVID-19 treatment space is intense, with many companies pursuing similar therapies. These factors make it essential to remain cautious, monitoring further developments and data closely.\"\n",
    ")\n",
    "\n",
    "print(f\"Sentiment Analysis Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23025de743370a0f",
   "metadata": {
    "collapsed": false,
    "id": "23025de743370a0f"
   },
   "source": [
    "# 3. Fine-Tuning Pre-trained Models\n",
    "While the pre-trained models provided by Hugging Face are powerful, you may want to fine-tune them for a specific task or dataset.\n",
    "\n",
    "Fine-tuning involves taking a pre-trained model and training it further on your own data. This can improve the model’s performance for specific use cases.\n",
    "\n",
    "\n",
    "\n",
    "### Step 1: Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b02c5a5d660751f6",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.684099300Z"
    },
    "id": "b02c5a5d660751f6"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Features, ClassLabel, Value\n",
    "import os, sys\n",
    "\n",
    "currentdir = os.getcwd()\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0, parentdir)\n",
    "data_file_path = os.path.join(parentdir, \"data/processed/finetuning_5_labels_topic_pruned.csv\") # choose the nr of labels urself\n",
    "if not os.path.exists(data_file_path):\n",
    "    raise FileNotFoundError(f\"The file {data_file_path} does not exist!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f27f5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALTERNATIVE FOR COLLAB\n",
    "from datasets import load_dataset, Features, ClassLabel, Value\n",
    "data_file_path = \"finetuning_5_labels_topic_pruned.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d805c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features\n",
    "features = Features(\n",
    "    {\n",
    "        \"text\": Value(\"string\"),\n",
    "        \"labels\": ClassLabel(names=[\"0.0\", \"1.0\", \"2.0\", \"3.0\", \"4.0\"]),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Load the dataset with the specified features\n",
    "dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files=data_file_path,\n",
    "    features=features,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a columns \"id\" with a unique row id\n",
    "dataset[\"train\"] = dataset[\"train\"].add_column(\"id\", range(len(dataset[\"train\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)\n",
    "print(dataset[\"train\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "labels = np.array(dataset[\"train\"][\"labels\"])\n",
    "\n",
    "# Generate indices for stratified split\n",
    "indices = np.arange(len(labels))\n",
    "train_indices, test_indices = train_test_split(\n",
    "    indices,\n",
    "    test_size=0.1,\n",
    "    stratify=labels,\n",
    "    random_state=42,  # Ensures reproducibility\n",
    ")\n",
    "\n",
    "# Create stratified train and test datasets, remember its a pandas dataframe\n",
    "train_dataset = dataset[\"train\"].select(train_indices)\n",
    "test_dataset = dataset[\"train\"].select(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573857f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming labels is a numpy array of your dataset labels\n",
    "labels = np.array(dataset[\"train\"][\"labels\"])\n",
    "label_counts = np.bincount(labels)\n",
    "label_percentages = label_counts / label_counts.sum() * 100 \n",
    "label_names = [\"Strongly Negative\",\"Negative\", \"Neutral\", \"Positive\", \"Strongly Positive\"]\n",
    "plt.bar(range(len(label_percentages)), label_percentages)\n",
    "plt.xlabel(\"Label\")\n",
    "plt.ylabel(\"Percentage (%)\") \n",
    "plt.title(\"Percentage Distribution of Labels\")  \n",
    "plt.xticks(range(len(label_percentages)), label_names)  \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now plot the distribution of text lengths by words\n",
    "\n",
    "text_lengths = [len(text.split()) for text in dataset[\"train\"][\"text\"]]\n",
    "plt.hist(text_lengths, bins=20)\n",
    "plt.xlabel(\"Text Length (Words)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Text Lengths\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404f2e557f938c88",
   "metadata": {
    "collapsed": false,
    "id": "404f2e557f938c88"
   },
   "source": [
    "### Step 2: Tokenize the Dataset\n",
    "The dataset needs to be tokenized before it can be fed into the model. Tokenization converts the text data into numerical format (tokens) that the model can process.\n",
    "\n",
    "We'll use the `AutoTokenizer` class from HuggingFace to tokenize the data. The `AutoTokenizer` class automatically selects the appropriate tokenizer for the model based on the `model_name`.\n",
    "\n",
    "Tokenization or transformation of the dataset can be done using the `map` method, which applies a function to all the elements of the dataset. This is easily done by defining a function that tokenizes the text data and then applying it to the dataset. When `batched=True`, the function will be applied to batches of data, which can improve performance by applying the function in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bfbf6e30ed6c40",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.687091400Z"
    },
    "id": "76bfbf6e30ed6c40"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer, padding=\"max_length\", max_length=512\n",
    ")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6454d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20413a582b139a0e",
   "metadata": {
    "collapsed": false,
    "id": "20413a582b139a0e"
   },
   "source": [
    "### Step 3: Load a Pre-trained Model\n",
    "Now that the data is tokenized, we'll load a pre-trained model that we'll fine-tune for sentiment classification.\n",
    "\n",
    "We need to import `AutoModelForSequenceClassification` for that. The key feature of this class is that it adds a classification head on top of the pre-trained transformer model to allow it to classify sequences into one or more categories (e.g., positive/negative sentiment, spam/ham, etc.). The `from_pretrained` method loads the pre-trained model with the specified configuration. The `num_labels` parameter specifies the number of labels in the classification task (binary in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c217ddcb27c998d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.689086Z"
    },
    "id": "7c217ddcb27c998d"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "lora_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels=5, # ignore_mismatched_sizes=True\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b466e12de8bfef",
   "metadata": {
    "collapsed": false,
    "id": "8b466e12de8bfef"
   },
   "source": [
    "### Step 4: Set Up the Trainer\n",
    "Hugging Face provides the Trainer class to help with the training and fine-tuning of models. We need to set up the trainer by providing the model, training arguments, and the datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91b8db4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average=\"weighted\", labels=[0, 1, 2, 3, 4]\n",
    "    )\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae5b805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(dataset[\"train\"][\"labels\"]),\n",
    "    y=dataset[\"train\"][\"labels\"],\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "print(\"Class Weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa808c9",
   "metadata": {},
   "source": [
    "***Loss function***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d052821d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Define custom loss function\n",
    "class WeightedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, class_weights):\n",
    "        super(WeightedCrossEntropyLoss, self).__init__()\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        return F.cross_entropy(logits, labels, weight=self.class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5dfd82",
   "metadata": {},
   "source": [
    "**System metric tracking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41234ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for tracking system metrics during training\n",
    "import time\n",
    "import psutil\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "def get_gpu_memory_usage():\n",
    "    # Run nvidia-smi to get both used and total memory\n",
    "    result = subprocess.run(\n",
    "        [\"nvidia-smi\", \"--query-gpu=memory.used,memory.total\", \"--format=csv,nounits,noheader\"],\n",
    "        stdout=subprocess.PIPE\n",
    "    )\n",
    "    used_memory, total_memory = result.stdout.decode(\"utf-8\").strip().split(\"\\n\")[0].split(\", \")\n",
    "\n",
    "    return int(used_memory), int(total_memory)\n",
    "\n",
    "# Pass the model type as a string\n",
    "def track_performance(model_type):\n",
    "    process = psutil.Process(os.getpid())\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Track memory and time\n",
    "    memory_usage = process.memory_info().rss / (1024 * 1024)\n",
    "    gpu_memory_used, gpu_memory_total = get_gpu_memory_usage()\n",
    "    end_time = time.time()\n",
    "\n",
    "    time_elapsed = end_time - start_time\n",
    "\n",
    "    print(f\"{model_type} - Time taken: {time_elapsed:.2f} seconds\")\n",
    "    print(f\"{model_type} - CPU Memory usage: {memory_usage:.2f} MB\")\n",
    "    print(f\"{model_type} - GPU Memory usage: {gpu_memory_used} MB / {gpu_memory_total} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78269d18",
   "metadata": {},
   "source": [
    "#### Defining the custom trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b17cfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "# Define custom trainer\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights, optimizer=None, num_training_steps=None, total_epochs=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "        self.optimizer = optimizer\n",
    "        self.num_training_steps = num_training_steps\n",
    "        self.total_epochs = total_epochs\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):  # Added **kwargs to handle required arguments unused by our loss\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = WeightedCrossEntropyLoss(self.class_weights.to(logits.device))\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def create_scheduler(self, num_training_steps=None, optimizer=None): # Slanted Triangular Learning Rate\n",
    "        if self.lr_scheduler is None:\n",
    "            num_training_steps = self.num_training_steps or (\n",
    "                len(self.train_dataset) // self.args.per_device_train_batch_size * self.args.num_train_epochs\n",
    "            )\n",
    "            self.lr_scheduler = OneCycleLR(\n",
    "                optimizer=optimizer,\n",
    "                max_lr=5e-5,  # Peak learning rate\n",
    "                total_steps=num_training_steps,\n",
    "                pct_start=0.3,  # Fraction of steps to increase LR for\n",
    "                anneal_strategy='cos',  # Linear decrease after peak\n",
    "            )\n",
    "        return self.lr_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c67a47",
   "metadata": {},
   "source": [
    "***Defining the training callback***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "746d76ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class TrainingMonitorCallback(TrainerCallback):\n",
    "    \"\"\" Tracks the learning rate and gradual unfreezing\"\"\"\n",
    "    def __init__(self, model, total_layers, plot_save_path=None):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.total_layers = total_layers\n",
    "        self.lrs = []\n",
    "        self.plot_save_path = plot_save_path\n",
    "\n",
    "    def on_step_end(self, args, state, control, optimizer, **kwargs):\n",
    "        \"\"\"Called at the end of each training step\"\"\"\n",
    "        if optimizer is not None:  # Safety check\n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "            self.lrs.append(lr)\n",
    "            \n",
    "    def plot_learning_rate(self):\n",
    "        \"\"\"Plot the learning rate history\"\"\"\n",
    "        if len(self.lrs) > 0:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(self.lrs)\n",
    "            plt.title('Learning Rate over Training Steps')\n",
    "            plt.xlabel('Training Step')\n",
    "            plt.ylabel('Learning Rate')\n",
    "            plt.grid(True)\n",
    "            if self.plot_save_path:\n",
    "                plt.savefig(self.plot_save_path)\n",
    "            else:\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230cef97",
   "metadata": {},
   "source": [
    "#### Trainer Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9436b5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"all\",\n",
    "    task_type=\"SEQ_CLS\"  # sequence classification\n",
    ")\n",
    "\n",
    "# Wrap the model with LoRA configuration\n",
    "lora_model = get_peft_model(lora_model, lora_config)\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "314f8aad08895c6e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.691080400Z"
    },
    "id": "314f8aad08895c6e"
   },
   "outputs": [],
   "source": [
    "# Create the callback\n",
    "tracker = TrainingMonitorCallback(lora_model, total_layers=12)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  # Output directory\n",
    "    eval_strategy=\"epoch\",  # Evaluate after each epoch\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,  # Batch size for training\n",
    "    per_device_eval_batch_size=16,  # Batch size for evaluation\n",
    "    gradient_accumulation_steps=2,  # Gradient accumulation steps FOR MEMORY EFFICIENCY\n",
    "    num_train_epochs=10,  # Number of epochs\n",
    "    load_best_model_at_end=True,  # Load the best model at the end\n",
    "    metric_for_best_model=\"f1\",  # Use F1 score to select the best model\n",
    "    save_total_limit=1,  # Limit the total amount of checkpoints\n",
    "    report_to=\"none\",  # Disable all integrations, including wandb\n",
    ")\n",
    "\n",
    "num_training_steps = (\n",
    "    (len(tokenized_train) // training_args.per_device_train_batch_size)\n",
    "    // training_args.gradient_accumulation_steps\n",
    ") * training_args.num_train_epochs\n",
    "\n",
    "\n",
    "custom_optimizer = torch.optim.AdamW(lora_model.parameters(), lr=2e-5, weight_decay=0.001)\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train, \n",
    "    eval_dataset=tokenized_test,  \n",
    "    compute_metrics=compute_metrics,\n",
    "    class_weights=class_weights,  \n",
    "    data_collator=data_collator,\n",
    "    optimizer=custom_optimizer,\n",
    "    num_training_steps=num_training_steps,\n",
    "    total_epochs=training_args.num_train_epochs,\n",
    ")\n",
    "\n",
    "trainer.optimizer = custom_optimizer\n",
    "trainer.optimizer = custom_optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4205eac4d06f8ae5",
   "metadata": {
    "collapsed": false,
    "id": "4205eac4d06f8ae5"
   },
   "source": [
    "### Step 5: Fine-tune the Model\n",
    "Now that the trainer is set up, we can start the fine-tuning process.\n",
    "\n",
    "Run the following cell to fine-tune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3125c17af30c4b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.693075300Z"
    },
    "id": "3c3125c17af30c4b"
   },
   "outputs": [],
   "source": [
    "# Learn without gradual unfreezing\n",
    "track_performance(\"LoRA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197d2fb4351c32ea",
   "metadata": {
    "collapsed": false,
    "id": "197d2fb4351c32ea"
   },
   "source": [
    "### Step 6: Evaluate the Model\n",
    "After training, we can evaluate the model’s performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d341d66e17736303",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.695070100Z"
    },
    "id": "d341d66e17736303"
   },
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation Results: {eval_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9efa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker.plot_learning_rate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1UJ8GcJSPhmt",
   "metadata": {
    "id": "1UJ8GcJSPhmt"
   },
   "source": [
    "### Step 7: Try out model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZTssehqcPd8R",
   "metadata": {
    "id": "ZTssehqcPd8R"
   },
   "outputs": [],
   "source": [
    "input_string = \"BioVie Announces Alignment with FDA on Clinical Trial to Assess Bezisterim in Parkinson’s Disease,SUNRISE-PD to evaluate the effect of bezisterim (NE3107) on motor and non-motor symptoms in ~60 patients with Parkinson’s disease who are naïve to carbidopa/levodopa,SUNRISE-PD to evaluate the effect of bezisterim (NE3107) on motor and non-motor symptoms in ~60 patients with Parkinson’s disease who are naïve to carbidopa/levodopa\"\n",
    "\n",
    "# Tokenize the input string\n",
    "inputs = tokenizer(input_string, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Get predictions (logits)\n",
    "with torch.no_grad():  # Disable gradient computation since we're just doing inference\n",
    "    outputs = lora_model(**inputs)\n",
    "    logits = outputs.logits\n",
    "print(logits)\n",
    "predicted_label = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "\n",
    "print(f\"Predicted label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53007a617ec9423",
   "metadata": {
    "collapsed": false,
    "id": "53007a617ec9423"
   },
   "source": [
    "### Step 8. Saving the Fine-tuned Model\n",
    "After training, it is often useful to save the fine-tuned model, so you can use it later without needing to re-train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e5c82ccd0913cc",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.699060Z"
    },
    "id": "a4e5c82ccd0913cc"
   },
   "outputs": [],
   "source": [
    "model.push_to_hub(\n",
    "    f\"akseljoonas/MedCPT-Article-Encoder-finetuned-{eval_results['f1']:.3f}\"\n",
    ")\n",
    "tokenizer.push_to_hub(\n",
    "    f\"akseljoonas/MedCPT-Article-Encoder-finetuned-{eval_results['f1']:.3f}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
