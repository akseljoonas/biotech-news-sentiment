{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abf508a8",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. [Set up the experiment](#1-set-up-the-experiment)\n",
    "   - [Import dependencies](#import-depencies)\n",
    "   - [Setup computing device](#setup-computing-device)\n",
    "\n",
    "2. [Define the model](#2-define-the-model)\n",
    "\n",
    "3. [Fine-Tuning Pre-trained Models](#3-fine-tuning-pre-trained-models)\n",
    "   - [Load Dataset from the file](#step-1-load-dataset-from-the-file)\n",
    "   - [Train Test Split](#step-2-train-test-split)\n",
    "   - [Plot the Data](#step-3-plot-the-data)\n",
    "   - [Tokenize the Dataset](#step-4-tokenize-the-dataset)\n",
    "   - [Load a Pre-trained Model](#step-5-load-a-pre-trained-model)\n",
    "   - [Set Up Training Functionality](#step-6-set-up-training-functionality)\n",
    "   - [Defining the custom Trainer](#step-7-defining-the-custom-trainer)\n",
    "   - [Instantiate the Trainer and Run Grid Search](#step-8-instantiate-the-trainer-and-run-grid-search)\n",
    "   - [Evaluate the Model](#step-9-evaluate-the-model)\n",
    "   - [Try out model](#step-10-try-out-model)\n",
    "   - [Saving the Fine-tuned Model](#step-11-saving-the-fine-tuned-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e8881b",
   "metadata": {},
   "source": [
    "# 1. Set up the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c56c5b",
   "metadata": {},
   "source": [
    "#### Import depencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397273d8a6e1aae7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T13:11:23.664156Z",
     "start_time": "2024-09-11T13:11:10.901679Z"
    },
    "id": "397273d8a6e1aae7"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset, Features, ClassLabel, Value\n",
    "import os\n",
    "import random\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f95367d",
   "metadata": {},
   "source": [
    "#### Setup computing device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69780a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    ")\n",
    "# if it is not MPS, try CUDA\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else device\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b78a7874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # If using GPUs\n",
    "\n",
    "set_seed(4242)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d241505041a6a8",
   "metadata": {
    "collapsed": false,
    "id": "b1d241505041a6a8"
   },
   "source": [
    "# 2. Define the model\n",
    "Sentiment Analysis is the task of classifying a given text into positive, negative, or neutral sentiments.\n",
    "\n",
    "In this example, we will use a pre-trained model for sentiment analysis. The `pipeline` will automatically download and load a model that has been pre-trained on a large dataset to perform this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e2aed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = \"ncbi/MedCPT-Article-Encoder\" #choose whichever u want\n",
    "#model_name = \"bert-base-uncased\"\n",
    "#model_name = \"dmis-lab/biobert-v1.1\"\n",
    "#model_name = \"microsoft/deberta-v3-base\"\n",
    "model_name = \"ProsusAI/finbert\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74849b2e",
   "metadata": {},
   "source": [
    "**Try out the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08c573b7d61fca0",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.666148400Z"
    },
    "id": "c08c573b7d61fca0"
   },
   "outputs": [],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\", model=model_name, device=device)\n",
    "result = classifier(\n",
    "    \"The development of a recombinant polyclonal antibody therapy for COVID-19 by GigaGen represents an early-stage positive news in response to a global health crisis. However, such initiatives often come with high risk and uncertainty given the complexity and time required for clinical trials and approval processes. Additionally, competition in the COVID-19 treatment space is intense, with many companies pursuing similar therapies. These factors make it essential to remain cautious, monitoring further developments and data closely.\"\n",
    ")\n",
    "\n",
    "print(f\"Sentiment Analysis Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23025de743370a0f",
   "metadata": {
    "collapsed": false,
    "id": "23025de743370a0f"
   },
   "source": [
    "# 3. Fine-Tuning Pre-trained Models\n",
    "While the pre-trained models provided by Hugging Face are powerful, you may want to fine-tune them for a specific task or dataset.\n",
    "\n",
    "Fine-tuning involves taking a pre-trained model and training it further on your own data. This can improve the model’s performance for specific use cases.\n",
    "\n",
    "### Step 1: Load Dataset from the file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759a5707",
   "metadata": {},
   "source": [
    "**Set the file path**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b02c5a5d660751f6",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.684099300Z"
    },
    "id": "b02c5a5d660751f6"
   },
   "outputs": [],
   "source": [
    "data_file_path = \"../data/processed/finetuning_5_labels_topic_pruned.csv\"\n",
    "if not os.path.exists(data_file_path):\n",
    "    raise FileNotFoundError(f\"The file {data_file_path} does not exist!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6faf7a",
   "metadata": {},
   "source": [
    "***ALTERNATIVE FOR COLLAB***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f27f5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_path = \"finetuning_5_labels_topic_pruned.csv\"\n",
    "if not os.path.exists(data_file_path):\n",
    "    raise FileNotFoundError(f\"The file {data_file_path} does not exist!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec2bdea",
   "metadata": {},
   "source": [
    "**Load the data with proper labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8d805c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features\n",
    "features = Features(\n",
    "    {\n",
    "        \"text\": Value(\"string\"),\n",
    "        \"labels\": ClassLabel(names=[\"0.0\", \"1.0\", \"2.0\", \"3.0\", \"4.0\"]),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Load the dataset with the specified features\n",
    "dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files=data_file_path,\n",
    "    features=features,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050eb1bf",
   "metadata": {},
   "source": [
    "### Step 2: Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3138634a",
   "metadata": {},
   "source": [
    "**Add a columns \"id\" with a unique row id for later use**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72621e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"] = dataset[\"train\"].add_column(\"id\", range(len(dataset[\"train\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e98cc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)\n",
    "print(dataset[\"train\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac28fa95",
   "metadata": {},
   "source": [
    "**Split the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "daf0e0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "labels = np.array(dataset[\"train\"][\"labels\"])\n",
    "indices = np.arange(len(labels))\n",
    "\n",
    "train_indices, test_indices = train_test_split(\n",
    "    indices,\n",
    "    test_size=0.1,\n",
    "    stratify=labels,\n",
    "    random_state=42,  # Ensures reproducibility\n",
    ")\n",
    "\n",
    "# Create stratified train and test datasets, remember its a pandas dataframe\n",
    "train_dataset = dataset[\"train\"].select(train_indices)\n",
    "test_dataset = dataset[\"train\"].select(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573857f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3d9534",
   "metadata": {},
   "source": [
    "### Step 3: Plot the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226a436e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(dataset[\"train\"][\"labels\"])\n",
    "label_counts = np.bincount(labels)\n",
    "label_percentages = label_counts / label_counts.sum() * 100  # Convert counts to percentages\n",
    "label_names = [\"Strongly Negative\",\"Negative\", \"Neutral\", \"Positive\", \"Strongly Positive\"]\n",
    "\n",
    "# Apply Seaborn style\n",
    "sns.set()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(label_percentages)), label_percentages, color='skyblue', edgecolor='black', alpha=0.8)\n",
    "plt.xlabel(\"Label\", fontsize=14, fontweight='normal')\n",
    "plt.ylabel(\"Percentage (%)\", fontsize=14, fontweight='normal')\n",
    "plt.title(\"Percentage Distribution of Labels\", fontsize=16, fontweight='normal')\n",
    "plt.xticks(range(len(label_percentages)), label_names, fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca8c7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_lengths = [len(text.split()) for text in dataset[\"train\"][\"text\"]]\n",
    "\n",
    "sns.set()\n",
    "\n",
    "# Create the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(text_lengths, bins=20, color='skyblue', edgecolor='black', alpha=0.8)\n",
    "plt.xlabel(\"Text Length (Words)\", fontsize=14, fontweight='normal')\n",
    "plt.ylabel(\"Frequency\", fontsize=14, fontweight='normal')\n",
    "plt.title(\"Distribution of Text Lengths\", fontsize=16, fontweight='normal')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404f2e557f938c88",
   "metadata": {
    "collapsed": false,
    "id": "404f2e557f938c88"
   },
   "source": [
    "### Step 4: Tokenize the Dataset\n",
    "The dataset needs to be tokenized before it can be fed into the model. Tokenization converts the text data into numerical format (tokens) that the model can process.\n",
    "\n",
    "We'll use the `AutoTokenizer` class from HuggingFace to tokenize the data. The `AutoTokenizer` class automatically selects the appropriate tokenizer for the model based on the `model_name`.\n",
    "\n",
    "Tokenization or transformation of the dataset can be done using the `map` method, which applies a function to all the elements of the dataset. This is easily done by defining a function that tokenizes the text data and then applying it to the dataset. When `batched=True`, the function will be applied to batches of data, which can improve performance by applying the function in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bfbf6e30ed6c40",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.687091400Z"
    },
    "id": "76bfbf6e30ed6c40"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer, padding=\"max_length\", max_length=512\n",
    ")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6454d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20413a582b139a0e",
   "metadata": {
    "collapsed": false,
    "id": "20413a582b139a0e"
   },
   "source": [
    "### Step 5: Load a Pre-trained Model\n",
    "Now that the data is tokenized, we'll load a pre-trained model that we'll fine-tune for sentiment classification.\n",
    "\n",
    "We'll use distilbert-base-uncased for this task.\n",
    "\n",
    "We need to import `AutoModelForSequenceClassification` for that. The key feature of this class is that it adds a classification head on top of the pre-trained transformer model to allow it to classify sequences into one or more categories (e.g., positive/negative sentiment, spam/ham, etc.). The `from_pretrained` method loads the pre-trained model with the specified configuration. The `num_labels` parameter specifies the number of labels in the classification task (binary in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c217ddcb27c998d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.689086Z"
    },
    "id": "7c217ddcb27c998d"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels=5, # ignore_mismatched_sizes=True\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b466e12de8bfef",
   "metadata": {
    "collapsed": false,
    "id": "8b466e12de8bfef"
   },
   "source": [
    "### Step 6: Set Up Training Functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afae434",
   "metadata": {},
   "source": [
    "**Setup the metrics calculation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91b8db4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average=\"weighted\", labels=[0, 1, 2, 3, 4]\n",
    "    )\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22dde5b",
   "metadata": {},
   "source": [
    "**Compute class weights to counter class inbalance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae5b805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(dataset[\"train\"][\"labels\"]),\n",
    "    y=dataset[\"train\"][\"labels\"],\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "print(\"Class Weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa808c9",
   "metadata": {},
   "source": [
    "**Define the Loss function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d052821d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Define custom loss function\n",
    "class WeightedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, class_weights):\n",
    "        super(WeightedCrossEntropyLoss, self).__init__()\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        return F.cross_entropy(logits, labels, weight=self.class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85519e83",
   "metadata": {},
   "source": [
    "**Define Gradual unfreezing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1917e05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradually_unfreeze(model, freeze_epoch, total_epochs):\n",
    "    total_layers = len(model.base_model.encoder.layer)\n",
    "    layers_to_unfreeze = int((freeze_epoch / total_epochs) * total_layers)\n",
    "    layers_to_unfreeze = min(layers_to_unfreeze, total_layers)\n",
    "\n",
    "    # Unfreeze the layers progressively from the bottom (earlier layers)\n",
    "    for i in range(total_layers - layers_to_unfreeze, total_layers):\n",
    "        for param in model.base_model.encoder.layer[i].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    print(f\"Epoch {freeze_epoch}: Unfreezing {layers_to_unfreeze}/{total_layers} layers.\")\n",
    "\n",
    "\n",
    "def freeze_all_layers(model):\n",
    "    # Freeze all transformer layers except the final classification layer\n",
    "    for param in model.base_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Unfreeze the final classifier layer\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5dfd82",
   "metadata": {},
   "source": [
    "**System metric tracking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "41234ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for tracking system metrics during training\n",
    "import time\n",
    "import psutil\n",
    "import subprocess\n",
    "\n",
    "def get_gpu_memory_usage():\n",
    "    # Run nvidia-smi to get both used and total memory ! MIGHT CAUSE CODE BREAK ON iOS MACHINES ! \n",
    "    result = subprocess.run(\n",
    "        [\"nvidia-smi\", \"--query-gpu=memory.used,memory.total\", \"--format=csv,nounits,noheader\"],\n",
    "        stdout=subprocess.PIPE\n",
    "    )\n",
    "    used_memory, total_memory = result.stdout.decode(\"utf-8\").strip().split(\"\\n\")[0].split(\", \")\n",
    "\n",
    "    return int(used_memory), int(total_memory)\n",
    "\n",
    "# Pass the model type as a string\n",
    "def track_performance(model_type):\n",
    "    process = psutil.Process(os.getpid())\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    trainer.train() # assumes trainer is already defined\n",
    "\n",
    "    # Track memory and time\n",
    "    memory_usage = process.memory_info().rss / (1024 * 1024)\n",
    "    gpu_memory_used, gpu_memory_total = get_gpu_memory_usage()\n",
    "    end_time = time.time()\n",
    "\n",
    "    time_elapsed = end_time - start_time\n",
    "\n",
    "    print(f\"{model_type} - Time taken: {time_elapsed:.2f} seconds\")\n",
    "    print(f\"{model_type} - CPU Memory usage: {memory_usage:.2f} MB\")\n",
    "    print(f\"{model_type} - GPU Memory usage: {gpu_memory_used} MB / {gpu_memory_total} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78269d18",
   "metadata": {},
   "source": [
    "### Step 7: Defining the custom Trainer\n",
    "Hugging Face provides the Trainer class to help with the training and fine-tuning of models. We need to set up the trainer by providing the model, training arguments, and the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad204a9b",
   "metadata": {},
   "source": [
    "**Defining the Trainer Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b17cfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "# Define custom trainer\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights, optimizer=None, num_training_steps=None, total_epochs=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "        self.optimizer = optimizer\n",
    "        self.num_training_steps = num_training_steps\n",
    "        self.total_epochs = total_epochs\n",
    "        self.freeze_epoch = 0\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):  # Added **kwargs to handle required arguments unused by our loss\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # Move class_weights to the same device as logits\n",
    "        loss_fct = WeightedCrossEntropyLoss(self.class_weights.to(logits.device))\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def create_scheduler(self, num_training_steps=None, optimizer=None): # Slanted Triangular Learning Rate\n",
    "        if self.lr_scheduler is None:\n",
    "            num_training_steps = self.num_training_steps or (\n",
    "                len(self.train_dataset) // self.args.per_device_train_batch_size * self.args.num_train_epochs\n",
    "            )\n",
    "            self.lr_scheduler = OneCycleLR(\n",
    "                optimizer=optimizer,\n",
    "                max_lr=1e-4,  # Peak learning rate\n",
    "                total_steps=num_training_steps,\n",
    "                pct_start=0.2,  # Fraction of steps to increase LR\n",
    "                anneal_strategy='cos',  # Linear decrease after peak\n",
    "            )\n",
    "        return self.lr_scheduler\n",
    "\n",
    "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
    "        # Gradually unfreeze layers based on the current epoch\n",
    "        gradually_unfreeze(self.model, self.freeze_epoch, self.total_epochs)\n",
    "        self.freeze_epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c67a47",
   "metadata": {},
   "source": [
    "**Defining the training callback**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "746d76ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class TrainingMonitorCallback(TrainerCallback):\n",
    "    \"\"\" Tracks the learning rate and gradual unfreezing\"\"\"\n",
    "    def __init__(self, model, total_layers, plot_save_path=None):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.total_layers = total_layers\n",
    "        self.lrs = []\n",
    "        self.plot_save_path = plot_save_path\n",
    "\n",
    "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
    "        \"\"\"Called at the beginning of each epoch\"\"\"\n",
    "        # Calculate how many layers are unfrozen based on the current epoch\n",
    "        gradually_unfreeze(self.model, state.epoch, args.num_train_epochs)\n",
    "        unfrozen_layers = sum(\n",
    "            1 for i in range(self.total_layers)\n",
    "            if any(param.requires_grad for param in self.model.base_model.encoder.layer[i].parameters())\n",
    "        )\n",
    "\n",
    "        # Print the number of unfrozen layers at the start of the epoch\n",
    "        print(f\"Epoch {state.epoch}: {unfrozen_layers}/{self.total_layers} encoder layers are unfrozen.\")\n",
    "\n",
    "    def on_step_end(self, args, state, control, optimizer, **kwargs):\n",
    "        \"\"\"Called at the end of each training step\"\"\"\n",
    "        if optimizer is not None:  # Safety check\n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "            self.lrs.append(lr)\n",
    "            \n",
    "    def plot_learning_rate(self):\n",
    "        \"\"\"Plot the learning rate history\"\"\"\n",
    "        if len(self.lrs) > 0:\n",
    "            sns.set()\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(self.lrs, color='skyblue', linewidth=2)\n",
    "            plt.title(\"Learning Rate over Training Steps\", fontsize=16, fontweight='normal')\n",
    "            plt.xlabel(\"Training Step\", fontsize=14, fontweight='normal')\n",
    "            plt.ylabel(\"Learning Rate\", fontsize=14, fontweight='normal')\n",
    "            plt.ticklabel_format(style='sci', axis='y', scilimits=(0, 0))\n",
    "            plt.grid(axis='both', linestyle='--', alpha=0.7)\n",
    "            plt.tight_layout()\n",
    "            if self.plot_save_path:\n",
    "                plt.savefig(self.plot_save_path)\n",
    "            else:\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230cef97",
   "metadata": {},
   "source": [
    "### Step 8: Instantiate the Trainer and Run Grid Search\n",
    "After adjusting the parameters tested in the grid search, run the following cell to execute fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "314f8aad08895c6e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.691080400Z"
    },
    "id": "314f8aad08895c6e"
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "# Define hyperparameters to search over\n",
    "learning_rates = [1e-5]\n",
    "batch_sizes = [8]\n",
    "num_epochs = [20]\n",
    "weight_decays = [0.01]\n",
    "\n",
    "# Create a grid of hyperparameter combinations\n",
    "param_grid = list(product(learning_rates, batch_sizes, num_epochs, weight_decays))\n",
    "\n",
    "best_model = None\n",
    "best_f1 = 0\n",
    "best_params = None\n",
    "\n",
    "# Iterate over all combinations\n",
    "for lr, batch_size, num_epochs, wd in param_grid:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5).to(device)\n",
    "    tracker = TrainingMonitorCallback(model, total_layers=len(model.base_model.encoder.layer))\n",
    "    # Create the custom optimizer\n",
    "    custom_optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "    # Create the TrainingArguments with the current hyperparameters\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",  # Output directory\n",
    "        eval_strategy=\"epoch\",  # Evaluate after each epoch\n",
    "        save_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=batch_size,  # Batch size for training\n",
    "        per_device_eval_batch_size=batch_size,  # Batch size for evaluation\n",
    "        gradient_accumulation_steps=2,  # Gradient accumulation steps FOR MEMORY EFFICIENCY\n",
    "        num_train_epochs=num_epochs,  # Number of epochs\n",
    "        load_best_model_at_end=True,  # Load the best model at the end\n",
    "        metric_for_best_model=\"f1\",  # Use F1 score to select the best model\n",
    "        save_total_limit=1,  # Limit the total amount of checkpoints\n",
    "        report_to=\"none\",  # Disable all integrations, including wandb\n",
    "    )\n",
    "\n",
    "    # Calculate number of training steps\n",
    "    num_training_steps = (\n",
    "        (len(tokenized_train) // training_args.per_device_train_batch_size)\n",
    "        // training_args.gradient_accumulation_steps\n",
    "    ) * training_args.num_train_epochs\n",
    "\n",
    "    # Create the Trainer\n",
    "    trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_test,\n",
    "        compute_metrics=compute_metrics,\n",
    "        class_weights=class_weights,\n",
    "        data_collator=data_collator,\n",
    "        optimizer=custom_optimizer,\n",
    "        num_training_steps=num_training_steps,\n",
    "        callbacks=[tracker],\n",
    "        total_epochs=training_args.num_train_epochs,\n",
    "    )\n",
    "    \n",
    "    trainer.optimizer = custom_optimizer\n",
    "    \n",
    "    freeze_all_layers(model)\n",
    "    #trainer.train()\n",
    "    track_performance(model_name)\n",
    "\n",
    "    # Evaluate and get the F1 score\n",
    "    f1_score = trainer.evaluate()[\"eval_f1\"]\n",
    "\n",
    "    # Store the best model and parameters based on F1 score\n",
    "    if f1_score > best_f1:\n",
    "        best_f1 = f1_score\n",
    "        best_model = trainer.model\n",
    "        best_params = (lr, batch_size, num_epochs, wd)\n",
    "        print(\"new best:\", best_params)\n",
    "        \n",
    "\n",
    "# Output the best parameters and model\n",
    "print(f\"Best F1 score: {best_f1}\")\n",
    "print(f\"Best hyperparameters: learning rate={best_params[0]}, batch size={best_params[1]}, epochs={best_params[2]}, weight decay={best_params[3]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197d2fb4351c32ea",
   "metadata": {
    "collapsed": false,
    "id": "197d2fb4351c32ea"
   },
   "source": [
    "### Step 9: Evaluate the Model\n",
    "After training, we can evaluate the model’s performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d341d66e17736303",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.695070100Z"
    },
    "id": "d341d66e17736303"
   },
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation Results: {eval_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9efa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker.plot_learning_rate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b223bfa2",
   "metadata": {},
   "source": [
    "**Generate Model Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c7bc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions\n",
    "#tokenized_dataset = dataset['train'].map(tokenize_function, batched=True)\n",
    "predictions_raw = trainer.predict(tokenized_test)\n",
    "predictions = np.argmax(predictions_raw.predictions, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899969b2",
   "metadata": {},
   "source": [
    "**Calculate Gross Profit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b739d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "class ProfitEvaluator:\n",
    "    def __init__(self, model_predictions, test_file, test_size=0.15, random_state=42):\n",
    "        self.predictions = model_predictions\n",
    "        self.base_dir = os.getcwd()\n",
    "        self.price_file = \"news+prices-new-2.csv\" # file with pricing information\n",
    "        self.test_file = test_file # file used to train the network\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "\n",
    "        self.eval_profit_dataset = pd.read_csv(self.price_file)\n",
    "        self.test_data = pd.read_csv(self.test_file)\n",
    "\n",
    "        self.test_indices = self.test_data.index.tolist()\n",
    "\n",
    "    \n",
    "    def calculate_price_change(self, current_index):\n",
    "        \"\"\" Calculates the price change for the current index \"\"\"\n",
    "        buy_in = self.eval_profit_dataset.loc[current_index, \"buy_in_price\"]\n",
    "        sell = self.eval_profit_dataset.loc[current_index, \"close\"]\n",
    "        increase_decrease = sell / buy_in - 1\n",
    "        return(increase_decrease)\n",
    "\n",
    "\n",
    "    def calculate_profit(self):\n",
    "        total_profit = 0\n",
    "        test_indices_copy = self.test_indices.copy()\n",
    "        prediction_transformation = {\n",
    "            0: -2, # Strong sell\n",
    "            1: -1, # Sell\n",
    "            2: 0, # Do nothing\n",
    "            3: 1, # Buy\n",
    "            4: 2, # Strong buy\n",
    "        }\n",
    "\n",
    "        for prediction in self.predictions:\n",
    "            if test_indices_copy:  # Check if there are still indices to process\n",
    "                current_index = test_indices_copy[0]  # Get the current index\n",
    "                price_move = self.calculate_price_change(current_index)\n",
    "                price_move *= prediction_transformation[prediction] # Apply the prediction transformation\n",
    "                total_profit += price_move\n",
    "\n",
    "                # Remove the processed index from the list\n",
    "                test_indices_copy.pop(0)\n",
    "            else:\n",
    "                print(\"No more indices left to process!\")\n",
    "                break\n",
    "\n",
    "        print(f\"\\nFinal Total Profit: {total_profit}\")\n",
    "        print(f\"Remaining rows in eval dataset: {len(test_indices_copy)}\")\n",
    "\n",
    "print(len(predictions))\n",
    "\n",
    "evaluator = ProfitEvaluator(predictions, data_file_path)\n",
    "evaluator.calculate_profit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a277ef",
   "metadata": {},
   "source": [
    "**Generate the ROC curves**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28b6839",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from torch.nn import functional as F\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "logits = torch.tensor(predictions_raw.predictions)\n",
    "\n",
    "# Apply softmax to logits to get probabilities for each class\n",
    "probabilities = F.softmax(logits, dim=-1)\n",
    "\n",
    "true_labels = predictions_raw.label_ids\n",
    "true_labels_bin = label_binarize(true_labels, classes=[0, 1, 2, 3, 4])\n",
    "\n",
    "sns.set()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "n_classes = true_labels_bin.shape[1] \n",
    "colors = sns.color_palette(\"husl\", n_classes)  \n",
    "\n",
    "for i in range(n_classes):\n",
    "    # For each class, calculate the ROC curve and AUC\n",
    "    fpr, tpr, _ = roc_curve(true_labels_bin[:, i], probabilities[:, i].cpu().numpy())\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.plot(fpr, tpr, lw=2, color=colors[i], label=f'Class {i} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "# Plot the random classifier line\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--', linewidth=1.5, label='Random Classifier')\n",
    "\n",
    "\n",
    "plt.xlabel(\"False Positive Rate\", fontsize=14, fontweight='normal')\n",
    "plt.ylabel(\"True Positive Rate\", fontsize=14, fontweight='normal')\n",
    "plt.title(\"Multi-Class ROC Curve\", fontsize=16, fontweight='normal')\n",
    "plt.legend(loc=\"lower right\", fontsize=12)\n",
    "plt.grid(axis='both', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1UJ8GcJSPhmt",
   "metadata": {
    "id": "1UJ8GcJSPhmt"
   },
   "source": [
    "### Step 10: Try out model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZTssehqcPd8R",
   "metadata": {
    "id": "ZTssehqcPd8R"
   },
   "outputs": [],
   "source": [
    "input_string = \"BioVie Announces Alignment with FDA on Clinical Trial to Assess Bezisterim in Parkinson’s Disease,SUNRISE-PD to evaluate the effect of bezisterim (NE3107) on motor and non-motor symptoms in ~60 patients with Parkinson’s disease who are naïve to carbidopa/levodopa,SUNRISE-PD to evaluate the effect of bezisterim (NE3107) on motor and non-motor symptoms in ~60 patients with Parkinson’s disease who are naïve to carbidopa/levodopa\"\n",
    "\n",
    "# Tokenize the input string\n",
    "inputs = tokenizer(input_string, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation since we're just doing inference\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "print(logits)\n",
    "predicted_label = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "\n",
    "print(f\"Predicted label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53007a617ec9423",
   "metadata": {
    "collapsed": false,
    "id": "53007a617ec9423"
   },
   "source": [
    "### Step 11: Saving the Fine-tuned Model\n",
    "After training, it is often useful to save the fine-tuned model, so you can use it later without needing to re-train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e5c82ccd0913cc",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.699060Z"
    },
    "id": "a4e5c82ccd0913cc"
   },
   "outputs": [],
   "source": [
    "model.push_to_hub(\n",
    "    f\"akseljoonas/MedCPT-Article-Encoder-finetuned-{eval_results['f1']:.3f}\"\n",
    ")\n",
    "tokenizer.push_to_hub(\n",
    "    f\"akseljoonas/MedCPT-Article-Encoder-finetuned-{eval_results['f1']:.3f}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
