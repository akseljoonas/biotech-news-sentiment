{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397273d8a6e1aae7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T13:11:23.664156Z",
     "start_time": "2024-09-11T13:11:10.901679Z"
    },
    "id": "397273d8a6e1aae7"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "device = (\n",
    "    torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    ")\n",
    "# if it is not MPS, try CUDA\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else device\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    # random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # If using GPUs\n",
    "\n",
    "set_seed(4242)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d241505041a6a8",
   "metadata": {
    "collapsed": false,
    "id": "b1d241505041a6a8"
   },
   "source": [
    "### Task 1: Sentiment Analysis\n",
    "Sentiment Analysis is the task of classifying a given text into positive, negative, or neutral sentiments.\n",
    "\n",
    "In this example, we will use a pre-trained model for sentiment analysis. The `pipeline` will automatically download and load a model that has been pre-trained on a large dataset to perform this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08c573b7d61fca0",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.666148400Z"
    },
    "id": "c08c573b7d61fca0"
   },
   "outputs": [],
   "source": [
    "#model_name = \"ncbi/MedCPT-Article-Encoder\"\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model_name, device=device)\n",
    "result = classifier(\n",
    "    \"The development of a recombinant polyclonal antibody therapy for COVID-19 by GigaGen represents an early-stage positive news in response to a global health crisis. However, such initiatives often come with high risk and uncertainty given the complexity and time required for clinical trials and approval processes. Additionally, competition in the COVID-19 treatment space is intense, with many companies pursuing similar therapies. These factors make it essential to remain cautious, monitoring further developments and data closely.\"\n",
    ")\n",
    "\n",
    "print(f\"Sentiment Analysis Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23025de743370a0f",
   "metadata": {
    "collapsed": false,
    "id": "23025de743370a0f"
   },
   "source": [
    "# 3. Fine-Tuning Pre-trained Models\n",
    "While the pre-trained models provided by Hugging Face are powerful, you may want to fine-tune them for a specific task or dataset.\n",
    "\n",
    "Fine-tuning involves taking a pre-trained model and training it further on your own data. This can improve the model’s performance for specific use cases.\n",
    "\n",
    "For this section, we’ll load the IMDB dataset (which contains movie reviews) and fine-tune a pre-trained model for sentiment classification.\n",
    "\n",
    "### Step 1: Load Dataset\n",
    "We'll use Hugging Face's datasets library to load the IMDB dataset.\n",
    "\n",
    "Datasets from the dataset library often come with pre-defined splits of the data, such as `train` and `test` sets.\n",
    "\n",
    "It is possible to filter or slice datasets to focus on specific subsets of the data, using the `select` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b02c5a5d660751f6",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.684099300Z"
    },
    "id": "b02c5a5d660751f6"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Features, ClassLabel, Value\n",
    "import os\n",
    "\n",
    "data_file_path = \"../data/processed/finetuning_5_labels_topic_pruned.csv\"\n",
    "if not os.path.exists(data_file_path):\n",
    "    raise FileNotFoundError(f\"The file {data_file_path} does not exist!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f27f5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALTERNATIVE FOR COLLAB\n",
    "from datasets import load_dataset, Features, ClassLabel, Value\n",
    "data_file_path = \"finetuning_5_labels_topic_pruned.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d805c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features\n",
    "features = Features(\n",
    "    {\n",
    "        \"text\": Value(\"string\"),\n",
    "        \"labels\": ClassLabel(names=[\"0.0\", \"1.0\", \"2.0\", \"3.0\", \"4.0\"]),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Load the dataset with the specified features\n",
    "dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files=data_file_path,\n",
    "    features=features,\n",
    ")\n",
    "\n",
    "\n",
    "#Now you can stratify by 'labels'\n",
    "#split_dataset = dataset[\"train\"].train_test_split(\n",
    "#    test_size=0.1, stratify_by_column=\"labels\"\n",
    "#)\n",
    "#train_dataset = split_dataset[\"train\"]\n",
    "#test_dataset = split_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050eb1bf",
   "metadata": {},
   "source": [
    "**Train Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72621e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a columns \"id\" with a unique row id\n",
    "dataset[\"train\"] = dataset[\"train\"].add_column(\"id\", range(len(dataset[\"train\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e98cc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)\n",
    "print(dataset[\"train\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daf0e0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "labels = np.array(dataset[\"train\"][\"labels\"])\n",
    "\n",
    "# Generate indices for stratified split\n",
    "indices = np.arange(len(labels))\n",
    "train_indices, test_indices = train_test_split(\n",
    "    indices,\n",
    "    test_size=0.1,\n",
    "    stratify=labels,\n",
    "    random_state=42,  # Ensures reproducibility\n",
    ")\n",
    "\n",
    "# Create stratified train and test datasets, remember its a pandas dataframe\n",
    "train_dataset = dataset[\"train\"].select(train_indices)\n",
    "test_dataset = dataset[\"train\"].select(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573857f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226a436e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming labels is a numpy array of your dataset labels\n",
    "labels = np.array(dataset[\"train\"][\"labels\"])\n",
    "label_counts = np.bincount(labels)\n",
    "label_percentages = label_counts / label_counts.sum() * 100  # Convert counts to percentages\n",
    "\n",
    "# Define label names\n",
    "label_names = [\"Strongly Negative\",\"Negative\", \"Neutral\", \"Positive\", \"Strongly Positive\"]\n",
    "\n",
    "# Plot the histogram\n",
    "plt.bar(range(len(label_percentages)), label_percentages)\n",
    "plt.xlabel(\"Label\")  # Set the x-axis label\n",
    "plt.ylabel(\"Percentage (%)\")  # Set the y-axis label\n",
    "plt.title(\"Percentage Distribution of Labels\")  # Set the title\n",
    "plt.xticks(range(len(label_percentages)), label_names)  # Map labels to names\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca8c7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now plot the distribution of text lengths by words\n",
    "\n",
    "text_lengths = [len(text.split()) for text in dataset[\"train\"][\"text\"]]\n",
    "plt.hist(text_lengths, bins=20)\n",
    "plt.xlabel(\"Text Length (Words)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Text Lengths\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404f2e557f938c88",
   "metadata": {
    "collapsed": false,
    "id": "404f2e557f938c88"
   },
   "source": [
    "### Step 2: Tokenize the Dataset\n",
    "The dataset needs to be tokenized before it can be fed into the model. Tokenization converts the text data into numerical format (tokens) that the model can process.\n",
    "\n",
    "We'll use the `AutoTokenizer` class from HuggingFace to tokenize the data. The `AutoTokenizer` class automatically selects the appropriate tokenizer for the model based on the `model_name`.\n",
    "\n",
    "Tokenization or transformation of the dataset can be done using the `map` method, which applies a function to all the elements of the dataset. This is easily done by defining a function that tokenizes the text data and then applying it to the dataset. When `batched=True`, the function will be applied to batches of data, which can improve performance by applying the function in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bfbf6e30ed6c40",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.687091400Z"
    },
    "id": "76bfbf6e30ed6c40"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer, padding=\"max_length\", max_length=512\n",
    ")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # print(examples[\"text\"][0])\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6454d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20413a582b139a0e",
   "metadata": {
    "collapsed": false,
    "id": "20413a582b139a0e"
   },
   "source": [
    "### Step 3: Load a Pre-trained Model\n",
    "Now that the data is tokenized, we'll load a pre-trained model that we'll fine-tune for sentiment classification.\n",
    "\n",
    "We'll use distilbert-base-uncased for this task.\n",
    "\n",
    "We need to import `AutoModelForSequenceClassification` for that. The key feature of this class is that it adds a classification head on top of the pre-trained transformer model to allow it to classify sequences into one or more categories (e.g., positive/negative sentiment, spam/ham, etc.). The `from_pretrained` method loads the pre-trained model with the specified configuration. The `num_labels` parameter specifies the number of labels in the classification task (binary in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c217ddcb27c998d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.689086Z"
    },
    "id": "7c217ddcb27c998d"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels=5, # ignore_mismatched_sizes=True\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b466e12de8bfef",
   "metadata": {
    "collapsed": false,
    "id": "8b466e12de8bfef"
   },
   "source": [
    "### Step 4: Set Up the Trainer\n",
    "Hugging Face provides the Trainer class to help with the training and fine-tuning of models. We need to set up the trainer by providing the model, training arguments, and the datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91b8db4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average=\"weighted\", labels=[0, 1, 2, 3, 4]\n",
    "    )\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae5b805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(dataset[\"train\"][\"labels\"]),\n",
    "    y=dataset[\"train\"][\"labels\"],\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "print(\"Class Weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa808c9",
   "metadata": {},
   "source": [
    "***Loss function***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d052821d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Define custom loss function\n",
    "class WeightedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, class_weights):\n",
    "        super(WeightedCrossEntropyLoss, self).__init__()\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        return F.cross_entropy(logits, labels, weight=self.class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85519e83",
   "metadata": {},
   "source": [
    "***Gradual unfreezing***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1917e05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradually_unfreeze(model, freeze_epoch, total_epochs):\n",
    "    total_layers = len(model.base_model.encoder.layer)\n",
    "    layers_to_unfreeze = int((freeze_epoch / total_epochs) * total_layers)\n",
    "    layers_to_unfreeze = min(layers_to_unfreeze, total_layers)\n",
    "\n",
    "    # Unfreeze the layers progressively from the bottom (earlier layers)\n",
    "    for i in range(total_layers - layers_to_unfreeze, total_layers):\n",
    "        for param in model.base_model.encoder.layer[i].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    print(f\"Epoch {freeze_epoch}: Unfreezing {layers_to_unfreeze}/{total_layers} layers.\")\n",
    "\n",
    "\n",
    "def freeze_all_layers(model):\n",
    "    # Freeze all transformer layers except the final classification layer\n",
    "    for param in model.base_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Unfreeze the final classifier layer\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5dfd82",
   "metadata": {},
   "source": [
    "**System metric tracking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41234ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for tracking system metrics during training\n",
    "import time\n",
    "import psutil\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "def get_gpu_memory_usage():\n",
    "    # Run nvidia-smi to get both used and total memory\n",
    "    result = subprocess.run(\n",
    "        [\"nvidia-smi\", \"--query-gpu=memory.used,memory.total\", \"--format=csv,nounits,noheader\"],\n",
    "        stdout=subprocess.PIPE\n",
    "    )\n",
    "    used_memory, total_memory = result.stdout.decode(\"utf-8\").strip().split(\"\\n\")[0].split(\", \")\n",
    "\n",
    "    return int(used_memory), int(total_memory)\n",
    "\n",
    "# Pass the model type as a string\n",
    "def track_performance(model_type):\n",
    "    process = psutil.Process(os.getpid())\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Track memory and time\n",
    "    memory_usage = process.memory_info().rss / (1024 * 1024)\n",
    "    gpu_memory_used, gpu_memory_total = get_gpu_memory_usage()\n",
    "    end_time = time.time()\n",
    "\n",
    "    time_elapsed = end_time - start_time\n",
    "\n",
    "    print(f\"{model_type} - Time taken: {time_elapsed:.2f} seconds\")\n",
    "    print(f\"{model_type} - CPU Memory usage: {memory_usage:.2f} MB\")\n",
    "    print(f\"{model_type} - GPU Memory usage: {gpu_memory_used} MB / {gpu_memory_total} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78269d18",
   "metadata": {},
   "source": [
    "#### Defining the custom trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b17cfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "# Define custom trainer\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights, optimizer=None, num_training_steps=None, total_epochs=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "        self.optimizer = optimizer\n",
    "        self.num_training_steps = num_training_steps\n",
    "        self.total_epochs = total_epochs\n",
    "        self.freeze_epoch = 0\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):  # Added **kwargs to handle required arguments unused by our loss\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # Move class_weights to the same device as logits\n",
    "        loss_fct = WeightedCrossEntropyLoss(self.class_weights.to(logits.device))\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def create_scheduler(self, num_training_steps=None, optimizer=None): # Slanted Triangular Learning Rate\n",
    "        if self.lr_scheduler is None:\n",
    "            num_training_steps = self.num_training_steps or (\n",
    "                len(self.train_dataset) // self.args.per_device_train_batch_size * self.args.num_train_epochs\n",
    "            )\n",
    "            self.lr_scheduler = OneCycleLR(\n",
    "                optimizer=optimizer,\n",
    "                max_lr=5e-5,  # Peak learning rate\n",
    "                total_steps=num_training_steps,\n",
    "                pct_start=0.3,  # Fraction of steps to increase LR\n",
    "                anneal_strategy='cos',  # Linear decrease after peak\n",
    "            )\n",
    "        return self.lr_scheduler\n",
    "\n",
    "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
    "        # Gradually unfreeze layers based on the current epoch\n",
    "        gradually_unfreeze(self.model, self.freeze_epoch, self.total_epochs)\n",
    "        self.freeze_epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c67a47",
   "metadata": {},
   "source": [
    "***Defining the training callback***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "746d76ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class TrainingMonitorCallback(TrainerCallback):\n",
    "    \"\"\" Tracks the learning rate and gradual unfreezing\"\"\"\n",
    "    def __init__(self, model, total_layers, plot_save_path=None):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.total_layers = total_layers\n",
    "        self.lrs = []\n",
    "        self.plot_save_path = plot_save_path\n",
    "\n",
    "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
    "        \"\"\"Called at the beginning of each epoch\"\"\"\n",
    "        # Calculate how many layers are unfrozen based on the current epoch\n",
    "        gradually_unfreeze(self.model, state.epoch, args.num_train_epochs)\n",
    "        unfrozen_layers = sum(\n",
    "            1 for i in range(self.total_layers)\n",
    "            if any(param.requires_grad for param in self.model.base_model.encoder.layer[i].parameters())\n",
    "        )\n",
    "\n",
    "        # Print the number of unfrozen layers at the start of the epoch\n",
    "        print(f\"Epoch {state.epoch}: {unfrozen_layers}/{self.total_layers} encoder layers are unfrozen.\")\n",
    "\n",
    "    def on_step_end(self, args, state, control, optimizer, **kwargs):\n",
    "        \"\"\"Called at the end of each training step\"\"\"\n",
    "        if optimizer is not None:  # Safety check\n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "            self.lrs.append(lr)\n",
    "            \n",
    "    def plot_learning_rate(self):\n",
    "        \"\"\"Plot the learning rate history\"\"\"\n",
    "        if len(self.lrs) > 0:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(self.lrs)\n",
    "            plt.title('Learning Rate over Training Steps')\n",
    "            plt.xlabel('Training Step')\n",
    "            plt.ylabel('Learning Rate')\n",
    "            plt.grid(True)\n",
    "            if self.plot_save_path:\n",
    "                plt.savefig(self.plot_save_path)\n",
    "            else:\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230cef97",
   "metadata": {},
   "source": [
    "#### Trainer Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "314f8aad08895c6e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.691080400Z"
    },
    "id": "314f8aad08895c6e"
   },
   "outputs": [],
   "source": [
    "# Create the callback\n",
    "tracker = TrainingMonitorCallback(model, total_layers=len(model.base_model.encoder.layer))\n",
    "custom_optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=0.001)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  # Output directory\n",
    "    eval_strategy=\"epoch\",  # Evaluate after each epoch\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,  # Batch size for training\n",
    "    per_device_eval_batch_size=16,  # Batch size for evaluation\n",
    "    gradient_accumulation_steps=2,  # Gradient accumulation steps FOR MEMORY EFFICIENCY\n",
    "    num_train_epochs=10,  # Number of epochs\n",
    "    load_best_model_at_end=True,  # Load the best model at the end\n",
    "    metric_for_best_model=\"f1\",  # Use F1 score to select the best model\n",
    "    save_total_limit=1,  # Limit the total amount of checkpoints\n",
    "    report_to=\"none\",  # Disable all integrations, including wandb\n",
    ")\n",
    "\n",
    "num_training_steps = (\n",
    "    (len(tokenized_train) // training_args.per_device_train_batch_size)\n",
    "    // training_args.gradient_accumulation_steps\n",
    ") * training_args.num_train_epochs\n",
    "\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    compute_metrics=compute_metrics,\n",
    "    class_weights=class_weights,\n",
    "    data_collator=data_collator,\n",
    "    optimizer=custom_optimizer,\n",
    "    num_training_steps=num_training_steps,\n",
    "    callbacks=[tracker],\n",
    "    total_epochs=training_args.num_train_epochs,\n",
    ")\n",
    "\n",
    "trainer.optimizer = custom_optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4205eac4d06f8ae5",
   "metadata": {
    "collapsed": false,
    "id": "4205eac4d06f8ae5"
   },
   "source": [
    "### Step 5: Fine-tune the Model\n",
    "Now that the trainer is set up, we can start the fine-tuning process.\n",
    "\n",
    "Run the following cell to fine-tune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3125c17af30c4b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.693075300Z"
    },
    "id": "3c3125c17af30c4b"
   },
   "outputs": [],
   "source": [
    "# Call this function before training starts to freeze all layers\n",
    "freeze_all_layers(model)\n",
    "track_performance(\"full-training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197d2fb4351c32ea",
   "metadata": {
    "collapsed": false,
    "id": "197d2fb4351c32ea"
   },
   "source": [
    "### Step 6: Evaluate the Model\n",
    "After training, we can evaluate the model’s performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d341d66e17736303",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.695070100Z"
    },
    "id": "d341d66e17736303"
   },
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation Results: {eval_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9efa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker.plot_learning_rate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1UJ8GcJSPhmt",
   "metadata": {
    "id": "1UJ8GcJSPhmt"
   },
   "source": [
    "### Step 7: Try out model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZTssehqcPd8R",
   "metadata": {
    "id": "ZTssehqcPd8R"
   },
   "outputs": [],
   "source": [
    "input_string = \"BioVie Announces Alignment with FDA on Clinical Trial to Assess Bezisterim in Parkinson’s Disease,SUNRISE-PD to evaluate the effect of bezisterim (NE3107) on motor and non-motor symptoms in ~60 patients with Parkinson’s disease who are naïve to carbidopa/levodopa,SUNRISE-PD to evaluate the effect of bezisterim (NE3107) on motor and non-motor symptoms in ~60 patients with Parkinson’s disease who are naïve to carbidopa/levodopa\"\n",
    "\n",
    "# Tokenize the input string\n",
    "inputs = tokenizer(input_string, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Get predictions (logits)\n",
    "with torch.no_grad():  # Disable gradient computation since we're just doing inference\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "print(logits)\n",
    "predicted_label = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "\n",
    "print(f\"Predicted label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53007a617ec9423",
   "metadata": {
    "collapsed": false,
    "id": "53007a617ec9423"
   },
   "source": [
    "### Step 8. Saving the Fine-tuned Model\n",
    "After training, it is often useful to save the fine-tuned model, so you can use it later without needing to re-train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e5c82ccd0913cc",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.699060Z"
    },
    "id": "a4e5c82ccd0913cc"
   },
   "outputs": [],
   "source": [
    "model.push_to_hub(\n",
    "    f\"akseljoonas/MedCPT-Article-Encoder-finetuned-{eval_results['f1']:.3f}\"\n",
    ")\n",
    "tokenizer.push_to_hub(\n",
    "    f\"akseljoonas/MedCPT-Article-Encoder-finetuned-{eval_results['f1']:.3f}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
