{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "397273d8a6e1aae7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T13:11:23.664156Z",
     "start_time": "2024-09-11T13:11:10.901679Z"
    },
    "id": "397273d8a6e1aae7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cpu\n",
      "False\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "device = (\n",
    "    torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    ")\n",
    "# if it is not MPS, try CUDA\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else device\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    # random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # If using GPUs\n",
    "\n",
    "set_seed(4242)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d241505041a6a8",
   "metadata": {
    "collapsed": false,
    "id": "b1d241505041a6a8"
   },
   "source": [
    "### Task 1: Sentiment Analysis\n",
    "Sentiment Analysis is the task of classifying a given text into positive, negative, or neutral sentiments.\n",
    "\n",
    "In this example, we will use a pre-trained model for sentiment analysis. The `pipeline` will automatically download and load a model that has been pre-trained on a large dataset to perform this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c08c573b7d61fca0",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.666148400Z"
    },
    "id": "c08c573b7d61fca0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis Result: [{'label': 'LABEL_1', 'score': 0.5116071701049805}]\n"
     ]
    }
   ],
   "source": [
    "#model_name = \"ncbi/MedCPT-Article-Encoder\"\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model_name, device=device)\n",
    "result = classifier(\n",
    "    \"The development of a recombinant polyclonal antibody therapy for COVID-19 by GigaGen represents an early-stage positive news in response to a global health crisis. However, such initiatives often come with high risk and uncertainty given the complexity and time required for clinical trials and approval processes. Additionally, competition in the COVID-19 treatment space is intense, with many companies pursuing similar therapies. These factors make it essential to remain cautious, monitoring further developments and data closely.\"\n",
    ")\n",
    "\n",
    "print(f\"Sentiment Analysis Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23025de743370a0f",
   "metadata": {
    "collapsed": false,
    "id": "23025de743370a0f"
   },
   "source": [
    "# 3. Fine-Tuning Pre-trained Models\n",
    "While the pre-trained models provided by Hugging Face are powerful, you may want to fine-tune them for a specific task or dataset.\n",
    "\n",
    "Fine-tuning involves taking a pre-trained model and training it further on your own data. This can improve the model’s performance for specific use cases.\n",
    "\n",
    "For this section, we’ll load the IMDB dataset (which contains movie reviews) and fine-tune a pre-trained model for sentiment classification.\n",
    "\n",
    "### Step 1: Load Dataset\n",
    "We'll use Hugging Face's datasets library to load the IMDB dataset.\n",
    "\n",
    "Datasets from the dataset library often come with pre-defined splits of the data, such as `train` and `test` sets.\n",
    "\n",
    "It is possible to filter or slice datasets to focus on specific subsets of the data, using the `select` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b02c5a5d660751f6",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.684099300Z"
    },
    "id": "b02c5a5d660751f6"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Features, ClassLabel, Value\n",
    "import os\n",
    "\n",
    "data_file_path = \"../data/processed/finetuning_5_labels_topic_pruned.csv\"\n",
    "if not os.path.exists(data_file_path):\n",
    "    raise FileNotFoundError(f\"The file {data_file_path} does not exist!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f27f5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALTERNATIVE FOR COLLAB\n",
    "from datasets import load_dataset, Features, ClassLabel, Value\n",
    "data_file_path = \"finetuning_5_labels_topic_pruned.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8d805c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features\n",
    "features = Features(\n",
    "    {\n",
    "        \"text\": Value(\"string\"),\n",
    "        \"labels\": ClassLabel(names=[\"0.0\", \"1.0\", \"2.0\", \"3.0\", \"4.0\"]),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Load the dataset with the specified features\n",
    "dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files=data_file_path,\n",
    "    features=features,\n",
    ")\n",
    "\n",
    "\n",
    "#Now you can stratify by 'labels'\n",
    "#split_dataset = dataset[\"train\"].train_test_split(\n",
    "#    test_size=0.1, stratify_by_column=\"labels\"\n",
    "#)\n",
    "#train_dataset = split_dataset[\"train\"]\n",
    "#test_dataset = split_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050eb1bf",
   "metadata": {},
   "source": [
    "**Train Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72621e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a columns \"id\" with a unique row id\n",
    "dataset[\"train\"] = dataset[\"train\"].add_column(\"id\", range(len(dataset[\"train\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e98cc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'labels', 'id'],\n",
      "        num_rows: 4020\n",
      "    })\n",
      "})\n",
      "{'text': 'GigaGen to Present at the Inaugural LifeSci Partners Private Company Virtual Summer Symposium SOUTH SAN FRANCISCO, Calif., July  29, 2020  (GLOBE NEWSWIRE) -- GigaGen Inc., a biotechnology company advancing transformative antibody drugs for infectious diseases, transplant rejection and checkpoint resistant cancers, today announced that it will participate in the LifeSci Partners Private Company Virtual Summer Symposium, taking place on August 4-5, 2020.', 'labels': 2, 'id': 1}\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "print(dataset[\"train\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "daf0e0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "labels = np.array(dataset[\"train\"][\"labels\"])\n",
    "\n",
    "# Generate indices for stratified split\n",
    "indices = np.arange(len(labels))\n",
    "train_indices, test_indices = train_test_split(\n",
    "    indices,\n",
    "    test_size=0.1,\n",
    "    stratify=labels,\n",
    "    random_state=42,  # Ensures reproducibility\n",
    ")\n",
    "\n",
    "# Create stratified train and test datasets, remember its a pandas dataframe\n",
    "train_dataset = dataset[\"train\"].select(train_indices)\n",
    "test_dataset = dataset[\"train\"].select(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "573857f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Cytek Biosciences to Participate in Upcoming Investor Conferences FREMONT, Calif., Oct.  30, 2024  (GLOBE NEWSWIRE) -- Cytek Biosciences, Inc. (Nasdaq CTKB), a leading cell analysis solutions company, today announced the company will be participating in the following investor conferences in November.',\n",
       " 'labels': 2,\n",
       " 'id': 3500}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "226a436e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming labels is a numpy array of your dataset labels\n",
    "labels = np.array(dataset[\"train\"][\"labels\"])\n",
    "label_counts = np.bincount(labels)\n",
    "label_percentages = label_counts / label_counts.sum() * 100  # Convert counts to percentages\n",
    "\n",
    "# Define label names\n",
    "label_names = [\"Strongly Negative\",\"Negative\", \"Neutral\", \"Positive\", \"Strongly Positive\"]\n",
    "\n",
    "# Plot the histogram\n",
    "plt.bar(range(len(label_percentages)), label_percentages)\n",
    "plt.xlabel(\"Label\")  # Set the x-axis label\n",
    "plt.ylabel(\"Percentage (%)\")  # Set the y-axis label\n",
    "plt.title(\"Percentage Distribution of Labels\")  # Set the title\n",
    "plt.xticks(range(len(label_percentages)), label_names)  # Map labels to names\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dca8c7ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Distribution of Text Lengths')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#now plot the distribution of text lengths by words\n",
    "\n",
    "text_lengths = [len(text.split()) for text in dataset[\"train\"][\"text\"]]\n",
    "plt.hist(text_lengths, bins=20)\n",
    "plt.xlabel(\"Text Length (Words)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Text Lengths\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404f2e557f938c88",
   "metadata": {
    "collapsed": false,
    "id": "404f2e557f938c88"
   },
   "source": [
    "### Step 2: Tokenize the Dataset\n",
    "The dataset needs to be tokenized before it can be fed into the model. Tokenization converts the text data into numerical format (tokens) that the model can process.\n",
    "\n",
    "We'll use the `AutoTokenizer` class from HuggingFace to tokenize the data. The `AutoTokenizer` class automatically selects the appropriate tokenizer for the model based on the `model_name`.\n",
    "\n",
    "Tokenization or transformation of the dataset can be done using the `map` method, which applies a function to all the elements of the dataset. This is easily done by defining a function that tokenizes the text data and then applying it to the dataset. When `batched=True`, the function will be applied to batches of data, which can improve performance by applying the function in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76bfbf6e30ed6c40",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.687091400Z"
    },
    "id": "76bfbf6e30ed6c40"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3618/3618 [00:00<00:00, 4069.33 examples/s]\n",
      "Map: 100%|██████████| 402/402 [00:00<00:00, 3977.30 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer, padding=\"max_length\", max_length=512\n",
    ")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # print(examples[\"text\"][0])\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6454d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'labels', 'id', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 402\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20413a582b139a0e",
   "metadata": {
    "collapsed": false,
    "id": "20413a582b139a0e"
   },
   "source": [
    "### Step 3: Load a Pre-trained Model\n",
    "Now that the data is tokenized, we'll load a pre-trained model that we'll fine-tune for sentiment classification.\n",
    "\n",
    "We'll use distilbert-base-uncased for this task.\n",
    "\n",
    "We need to import `AutoModelForSequenceClassification` for that. The key feature of this class is that it adds a classification head on top of the pre-trained transformer model to allow it to classify sequences into one or more categories (e.g., positive/negative sentiment, spam/ham, etc.). The `from_pretrained` method loads the pre-trained model with the specified configuration. The `num_labels` parameter specifies the number of labels in the classification task (binary in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c217ddcb27c998d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.689086Z"
    },
    "id": "7c217ddcb27c998d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels=5, # ignore_mismatched_sizes=True\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b466e12de8bfef",
   "metadata": {
    "collapsed": false,
    "id": "8b466e12de8bfef"
   },
   "source": [
    "### Step 4: Set Up the Trainer\n",
    "Hugging Face provides the Trainer class to help with the training and fine-tuning of models. We need to set up the trainer by providing the model, training arguments, and the datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91b8db4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average=\"weighted\", labels=[0, 1, 2, 3, 4]\n",
    "    )\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bae5b805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: tensor([2.6800, 1.9901, 0.3109, 2.0050, 2.4438])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(dataset[\"train\"][\"labels\"]),\n",
    "    y=dataset[\"train\"][\"labels\"],\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "print(\"Class Weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa808c9",
   "metadata": {},
   "source": [
    "***Loss function***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d052821d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Define custom loss function\n",
    "class WeightedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, class_weights):\n",
    "        super(WeightedCrossEntropyLoss, self).__init__()\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        return F.cross_entropy(logits, labels, weight=self.class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85519e83",
   "metadata": {},
   "source": [
    "***Gradual unfreezing***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1917e05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradually_unfreeze(model, freeze_epoch, total_epochs):\n",
    "    total_layers = len(model.base_model.encoder.layer)\n",
    "    layers_to_unfreeze = int((freeze_epoch / total_epochs) * total_layers)\n",
    "    layers_to_unfreeze = min(layers_to_unfreeze, total_layers)\n",
    "\n",
    "    # Unfreeze the layers progressively from the bottom (earlier layers)\n",
    "    for i in range(total_layers - layers_to_unfreeze, total_layers):\n",
    "        for param in model.base_model.encoder.layer[i].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    print(f\"Epoch {freeze_epoch}: Unfreezing {layers_to_unfreeze}/{total_layers} layers.\")\n",
    "\n",
    "\n",
    "def freeze_all_layers(model):\n",
    "    # Freeze all transformer layers except the final classification layer\n",
    "    for param in model.base_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Unfreeze the final classifier layer\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5dfd82",
   "metadata": {},
   "source": [
    "**System metric tracking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "41234ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for tracking system metrics during training\n",
    "import time\n",
    "import psutil\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "def get_gpu_memory_usage():\n",
    "    # Run nvidia-smi to get both used and total memory\n",
    "    result = subprocess.run(\n",
    "        [\"nvidia-smi\", \"--query-gpu=memory.used,memory.total\", \"--format=csv,nounits,noheader\"],\n",
    "        stdout=subprocess.PIPE\n",
    "    )\n",
    "    used_memory, total_memory = result.stdout.decode(\"utf-8\").strip().split(\"\\n\")[0].split(\", \")\n",
    "\n",
    "    return int(used_memory), int(total_memory)\n",
    "\n",
    "# Pass the model type as a string\n",
    "def track_performance(model_type):\n",
    "    process = psutil.Process(os.getpid())\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Track memory and time\n",
    "    memory_usage = process.memory_info().rss / (1024 * 1024)\n",
    "    gpu_memory_used, gpu_memory_total = get_gpu_memory_usage()\n",
    "    end_time = time.time()\n",
    "\n",
    "    time_elapsed = end_time - start_time\n",
    "\n",
    "    print(f\"{model_type} - Time taken: {time_elapsed:.2f} seconds\")\n",
    "    print(f\"{model_type} - CPU Memory usage: {memory_usage:.2f} MB\")\n",
    "    print(f\"{model_type} - GPU Memory usage: {gpu_memory_used} MB / {gpu_memory_total} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78269d18",
   "metadata": {},
   "source": [
    "#### Defining the custom trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b17cfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "# Define custom trainer\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights, optimizer=None, num_training_steps=None, total_epochs=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "        self.optimizer = optimizer\n",
    "        self.num_training_steps = num_training_steps\n",
    "        self.total_epochs = total_epochs\n",
    "        self.freeze_epoch = 0\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):  # Added **kwargs to handle required arguments unused by our loss\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # Move class_weights to the same device as logits\n",
    "        loss_fct = WeightedCrossEntropyLoss(self.class_weights.to(logits.device))\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def create_scheduler(self, num_training_steps=None, optimizer=None): # Slanted Triangular Learning Rate\n",
    "        if self.lr_scheduler is None:\n",
    "            num_training_steps = self.num_training_steps or (\n",
    "                len(self.train_dataset) // self.args.per_device_train_batch_size * self.args.num_train_epochs\n",
    "            )\n",
    "            self.lr_scheduler = OneCycleLR(\n",
    "                optimizer=optimizer,\n",
    "                max_lr=5e-5,  # Peak learning rate\n",
    "                total_steps=num_training_steps,\n",
    "                pct_start=0.3,  # Fraction of steps to increase LR\n",
    "                anneal_strategy='cos',  # Linear decrease after peak\n",
    "            )\n",
    "        return self.lr_scheduler\n",
    "\n",
    "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
    "        # Gradually unfreeze layers based on the current epoch\n",
    "        gradually_unfreeze(self.model, self.freeze_epoch, self.total_epochs)\n",
    "        self.freeze_epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c67a47",
   "metadata": {},
   "source": [
    "***Defining the training callback***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "746d76ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class TrainingMonitorCallback(TrainerCallback):\n",
    "    \"\"\" Tracks the learning rate and gradual unfreezing\"\"\"\n",
    "    def __init__(self, model, total_layers, plot_save_path=None):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.total_layers = total_layers\n",
    "        self.lrs = []\n",
    "        self.plot_save_path = plot_save_path\n",
    "\n",
    "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
    "        \"\"\"Called at the beginning of each epoch\"\"\"\n",
    "        # Calculate how many layers are unfrozen based on the current epoch\n",
    "        gradually_unfreeze(self.model, state.epoch, args.num_train_epochs)\n",
    "        unfrozen_layers = sum(\n",
    "            1 for i in range(self.total_layers)\n",
    "            if any(param.requires_grad for param in self.model.base_model.encoder.layer[i].parameters())\n",
    "        )\n",
    "\n",
    "        # Print the number of unfrozen layers at the start of the epoch\n",
    "        print(f\"Epoch {state.epoch}: {unfrozen_layers}/{self.total_layers} encoder layers are unfrozen.\")\n",
    "\n",
    "    def on_step_end(self, args, state, control, optimizer, **kwargs):\n",
    "        \"\"\"Called at the end of each training step\"\"\"\n",
    "        if optimizer is not None:  # Safety check\n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "            self.lrs.append(lr)\n",
    "            \n",
    "    def plot_learning_rate(self):\n",
    "        \"\"\"Plot the learning rate history\"\"\"\n",
    "        if len(self.lrs) > 0:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(self.lrs)\n",
    "            plt.title('Learning Rate over Training Steps')\n",
    "            plt.xlabel('Training Step')\n",
    "            plt.ylabel('Learning Rate')\n",
    "            plt.grid(True)\n",
    "            if self.plot_save_path:\n",
    "                plt.savefig(self.plot_save_path)\n",
    "            else:\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230cef97",
   "metadata": {},
   "source": [
    "#### Trainer Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "314f8aad08895c6e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.691080400Z"
    },
    "id": "314f8aad08895c6e"
   },
   "outputs": [],
   "source": [
    "# Create the callback\n",
    "tracker = TrainingMonitorCallback(model, total_layers=len(model.base_model.encoder.layer))\n",
    "custom_optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=0.001)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  # Output directory\n",
    "    eval_strategy=\"epoch\",  # Evaluate after each epoch\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,  # Batch size for training\n",
    "    per_device_eval_batch_size=16,  # Batch size for evaluation\n",
    "    gradient_accumulation_steps=2,  # Gradient accumulation steps FOR MEMORY EFFICIENCY\n",
    "    num_train_epochs=10,  # Number of epochs\n",
    "    load_best_model_at_end=True,  # Load the best model at the end\n",
    "    metric_for_best_model=\"f1\",  # Use F1 score to select the best model\n",
    "    save_total_limit=1,  # Limit the total amount of checkpoints\n",
    "    report_to=\"none\",  # Disable all integrations, including wandb\n",
    ")\n",
    "\n",
    "num_training_steps = (\n",
    "    (len(tokenized_train) // training_args.per_device_train_batch_size)\n",
    "    // training_args.gradient_accumulation_steps\n",
    ") * training_args.num_train_epochs\n",
    "\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    compute_metrics=compute_metrics,\n",
    "    class_weights=class_weights,\n",
    "    data_collator=data_collator,\n",
    "    optimizer=custom_optimizer,\n",
    "    num_training_steps=num_training_steps,\n",
    "    callbacks=[tracker],\n",
    "    total_epochs=training_args.num_train_epochs,\n",
    ")\n",
    "\n",
    "trainer.optimizer = custom_optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4205eac4d06f8ae5",
   "metadata": {
    "collapsed": false,
    "id": "4205eac4d06f8ae5"
   },
   "source": [
    "### Step 5: Fine-tune the Model\n",
    "Now that the trainer is set up, we can start the fine-tuning process.\n",
    "\n",
    "Run the following cell to fine-tune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c3125c17af30c4b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.693075300Z"
    },
    "id": "3c3125c17af30c4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Unfreezing 0/12 layers.\n",
      "Epoch 0: 0/12 encoder layers are unfrozen.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='1130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   4/1130 00:13 < 2:08:21, 0.15 it/s, Epoch 0.03/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='23' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [23/26 00:46 < 00:06, 0.47 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Call this function before training starts to freeze all layers\u001b[39;00m\n\u001b[0;32m      2\u001b[0m freeze_all_layers(model)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtrack_performance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfull-training\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[24], line 23\u001b[0m, in \u001b[0;36mtrack_performance\u001b[1;34m(model_type)\u001b[0m\n\u001b[0;32m     19\u001b[0m process \u001b[38;5;241m=\u001b[39m psutil\u001b[38;5;241m.\u001b[39mProcess(os\u001b[38;5;241m.\u001b[39mgetpid())\n\u001b[0;32m     21\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 23\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Track memory and time\u001b[39;00m\n\u001b[0;32m     26\u001b[0m memory_usage \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mmemory_info()\u001b[38;5;241m.\u001b[39mrss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m)\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2169\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2531\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2524\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2525\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2527\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2528\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2529\u001b[0m )\n\u001b[0;32m   2530\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2531\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2534\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2535\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2536\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2537\u001b[0m ):\n\u001b[0;32m   2538\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2539\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3678\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3676\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs)\n\u001b[0;32m   3677\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3678\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3680\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   3681\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3682\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3683\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   3684\u001b[0m ):\n",
      "Cell \u001b[1;32mIn[25], line 16\u001b[0m, in \u001b[0;36mCustomTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, **kwargs)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, inputs, return_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):  \u001b[38;5;66;03m# Added **kwargs to handle required arguments unused by our loss\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# Move class_weights to the same device as logits\u001b[39;00m\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1665\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1657\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1658\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1659\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1660\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1661\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1662\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1663\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1665\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1671\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1672\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1673\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1675\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1677\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1679\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    686\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    692\u001b[0m         output_attentions,\n\u001b[0;32m    693\u001b[0m     )\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    575\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    582\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    584\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 585\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    592\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    594\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:515\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    507\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    513\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 515\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    524\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    525\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440\u001b[0m, in \u001b[0;36mBertSdpaSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;66;03m# We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\u001b[39;00m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;66;03m# in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;66;03m# The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;66;03m# a causal mask in case tgt_len == 1.\u001b[39;00m\n\u001b[0;32m    436\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    437\u001b[0m     \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention \u001b[38;5;129;01mand\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tgt_len \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    438\u001b[0m )\n\u001b[1;32m--> 440\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout_prob\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    449\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    450\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(bsz, tgt_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_head_size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Call this function before training starts to freeze all layers\n",
    "freeze_all_layers(model)\n",
    "track_performance(\"full-training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197d2fb4351c32ea",
   "metadata": {
    "collapsed": false,
    "id": "197d2fb4351c32ea"
   },
   "source": [
    "### Step 6: Evaluate the Model\n",
    "After training, we can evaluate the model’s performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d341d66e17736303",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.695070100Z"
    },
    "id": "d341d66e17736303"
   },
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation Results: {eval_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9efa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker.plot_learning_rate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c7bc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions\n",
    "tokenized_dataset = dataset['train'].map(tokenize_function, batched=True)\n",
    "predictions = trainer.predict(tokenized_dataset).predictions\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "model_predictions = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0b739d49",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 55\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFinal Total % Profit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_profit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemaining rows in eval dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_indices_copy)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 55\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(predictions)\n\u001b[0;32m     58\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m ProfitEvaluator(model_predictions, data_file_path)\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\transformers\\trainer.py:4154\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[1;34m(self, test_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4151\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   4153\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 4154\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPrediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\n\u001b[0;32m   4156\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4157\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   4158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\transformers\\trainer.py:4270\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4267\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[0;32m   4269\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[1;32m-> 4270\u001b[0m losses, logits, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4271\u001b[0m main_input_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain_input_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   4272\u001b[0m inputs_decode \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   4273\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs[main_input_name]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4274\u001b[0m )\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\transformers\\trainer.py:4486\u001b[0m, in \u001b[0;36mTrainer.prediction_step\u001b[1;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[0;32m   4484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_labels \u001b[38;5;129;01mor\u001b[39;00m loss_without_labels:\n\u001b[0;32m   4485\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 4486\u001b[0m         loss, outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   4487\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m   4489\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m):\n",
      "Cell \u001b[1;32mIn[25], line 16\u001b[0m, in \u001b[0;36mCustomTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, **kwargs)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, inputs, return_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):  \u001b[38;5;66;03m# Added **kwargs to handle required arguments unused by our loss\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# Move class_weights to the same device as logits\u001b[39;00m\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1665\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1657\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1658\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1659\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1660\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1661\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1662\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1663\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1665\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1671\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1672\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1673\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1675\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1677\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1679\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    686\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    692\u001b[0m         output_attentions,\n\u001b[0;32m    693\u001b[0m     )\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:627\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    624\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    625\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 627\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    630\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    632\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\transformers\\pytorch_utils.py:255\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 255\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:639\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m--> 639\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    640\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[0;32m    641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 539\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    540\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[0;32m    541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\.UNI\\LTP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "class ProfitEvaluator:\n",
    "    def __init__(self, model_predictions, test_file, test_size=0.15, random_state=42):\n",
    "        self.predictions = model_predictions\n",
    "        self.base_dir = os.getcwd()\n",
    "        #price_file = \"../data/processed/news+prices-new-2.csv\"\n",
    "        price_file = \"news+prices-new-2.csv\"\n",
    "        self.price_file = price_file # file with pricing information\n",
    "        self.test_file = test_file # file used to train the network\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "\n",
    "        self.eval_profit_dataset = pd.read_csv(self.price_file)\n",
    "        self.test_data = pd.read_csv(self.test_file)\n",
    "\n",
    "        self.test_indices = self.test_data.index.tolist()\n",
    "\n",
    "    \n",
    "    def calculate_price_change(self, current_index):\n",
    "        \"\"\" Calculates the price change for the current index \"\"\"\n",
    "        buy_in = self.eval_profit_dataset.loc[current_index, \"buy_in_price\"]\n",
    "        sell = self.eval_profit_dataset.loc[current_index, \"close\"]\n",
    "        increase_decrease = sell / buy_in - 1\n",
    "        return(increase_decrease)\n",
    "\n",
    "\n",
    "    def calculate_profit(self):\n",
    "        total_profit = 0\n",
    "        test_indices_copy = self.test_indices.copy()\n",
    "\n",
    "        for prediction in self.predictions:\n",
    "            if test_indices_copy:  # Check if there are still indices to process\n",
    "                current_index = test_indices_copy[0]  # Get the current index\n",
    "                price_move = self.calculate_price_change(current_index)\n",
    "\n",
    "                if prediction == 1:\n",
    "                  total_profit -= price_move\n",
    "                elif prediction == 0:\n",
    "                  total_profit -= 2*price_move\n",
    "                elif prediction == 3:\n",
    "                    total_profit += price_move\n",
    "                elif prediction == 4:\n",
    "                    total_profit += 2*price_move\n",
    "                # For prediction == 1, no profit adjustment happens\n",
    "                \n",
    "                #print(f\"Total Profit: {total_profit} (Prediction: {prediction}, Price Move: {price_move})\")\n",
    "                \n",
    "                # Remove the processed index from the list\n",
    "                test_indices_copy.pop(0)\n",
    "            else:\n",
    "                print(\"No more indices left to process!\")\n",
    "                break\n",
    "\n",
    "        print(f\"\\nFinal Total % Profit: {total_profit}\")\n",
    "        print(f\"Remaining rows in eval dataset: {len(test_indices_copy)}\")\n",
    "\n",
    "print(model_predictions[:100])\n",
    "\n",
    "evaluator = ProfitEvaluator(model_predictions, data_file_path)\n",
    "evaluator.calculate_profit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1UJ8GcJSPhmt",
   "metadata": {
    "id": "1UJ8GcJSPhmt"
   },
   "source": [
    "### Step 7: Try out model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZTssehqcPd8R",
   "metadata": {
    "id": "ZTssehqcPd8R"
   },
   "outputs": [],
   "source": [
    "input_string = \"BioVie Announces Alignment with FDA on Clinical Trial to Assess Bezisterim in Parkinson’s Disease,SUNRISE-PD to evaluate the effect of bezisterim (NE3107) on motor and non-motor symptoms in ~60 patients with Parkinson’s disease who are naïve to carbidopa/levodopa,SUNRISE-PD to evaluate the effect of bezisterim (NE3107) on motor and non-motor symptoms in ~60 patients with Parkinson’s disease who are naïve to carbidopa/levodopa\"\n",
    "\n",
    "# Tokenize the input string\n",
    "inputs = tokenizer(input_string, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Get predictions (logits)\n",
    "with torch.no_grad():  # Disable gradient computation since we're just doing inference\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "print(logits)\n",
    "predicted_label = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "\n",
    "print(f\"Predicted label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53007a617ec9423",
   "metadata": {
    "collapsed": false,
    "id": "53007a617ec9423"
   },
   "source": [
    "### Step 8. Saving the Fine-tuned Model\n",
    "After training, it is often useful to save the fine-tuned model, so you can use it later without needing to re-train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e5c82ccd0913cc",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.699060Z"
    },
    "id": "a4e5c82ccd0913cc"
   },
   "outputs": [],
   "source": [
    "model.push_to_hub(\n",
    "    f\"akseljoonas/MedCPT-Article-Encoder-finetuned-{eval_results['f1']:.3f}\"\n",
    ")\n",
    "tokenizer.push_to_hub(\n",
    "    f\"akseljoonas/MedCPT-Article-Encoder-finetuned-{eval_results['f1']:.3f}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
